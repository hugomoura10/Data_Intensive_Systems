{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation import *\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {'log_in': ['one',['credentials check','sing up','recover pw and log in'],(0,10)],\n",
    "            'search_book': ['rand',['history','fantasy','crime','poetry','biography'],(5,15)],\n",
    "            'shipment' : ['all',['adress','door number','zip code'],(15,50)],\n",
    "            'payment' : ['one',['visa','master card','revolut','paypal','apple pay'],(10,100)],\n",
    "            'new_site' : ['con',['site1','site2'],(10,100)],\n",
    "            'site1' : ['opt',['site1_1','site1_2','site1_3'],(10,100)],\n",
    "            'site2' : ['opt',['site2_1','site2_2'],(10,100)]\n",
    "}   \n",
    "start_time = datetime(2024, 6, 3, 9, 0, 0)  \n",
    "end_time = datetime(2024, 6, 3, 10, 45, 0) \n",
    " \n",
    "#generate_dataset(tasks, 1000000,start_time,end_time,file_name=\"dataset\")\n",
    "#data = spark.read.csv(\"data/SDG_dataset.csv\", header=True, inferSchema=True)  \n",
    "generate_dataset(tasks, 100000,start_time,end_time,file_name=\"dataset_test\")  \n",
    "data = spark.read.csv(\"data/SDG_dataset_test.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average trace length: 38.158701120512234\n",
      "average trace # shingles: 32.158701120512234\n"
     ]
    }
   ],
   "source": [
    "def shingle(text, k=7):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text)-k +1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return list(set(shingle_set))\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_filtered1_7 = data.filter(data.type.isin(['Req']))\n",
    "df_grouped1_7 = df_filtered1_7.groupBy(\"user_id\").agg(concat_ws(\"\",collect_list(\"to\")).alias(\"features\"))\n",
    "\n",
    "shingles_udf1_7 = udf(shingle, ArrayType(StringType()))\n",
    "df_shingles1_7 = df_filtered1_7.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"trace\")) \\\n",
    "    .withColumn(\"shingles\", shingles_udf1_7(col(\"trace\"))) \\\n",
    "    .select(\"user_id\", \"shingles\")\n",
    "\n",
    "average_length1_7 = df_grouped1_7.select(avg(length(col('features')))).collect()[0][0]\n",
    "average_shingles7_1 = df_shingles1_7.withColumn(\"list_length\", size(col(\"shingles\"))) \\\n",
    "                     .agg(avg(\"list_length\").alias(\"average_list_length\")).collect()[0][0]\n",
    "\n",
    "print('average trace length:',average_length1_7)\n",
    "print('average trace # shingles:',average_shingles7_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of cases: 4373\n",
      "Number of unique processes after merging them with 0.96875 threshold using 7-shingles: 3615\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial number of cases: {df_grouped1_7.count()}\")\n",
    "threshold = (int(average_shingles7_1)-1)/int(average_shingles7_1)\n",
    "ans = minhash_lsh(df_grouped1_7,7,threshold)\n",
    "replacement_candidates1_7, minhash_dic1_7 = ans[0],ans[1]\n",
    "new_process_dictionary1_7= bucketing(replacement_candidates1_7)\n",
    "print(f\"Number of unique processes after merging them with {threshold} threshold using 7-shingles: {len(new_process_dictionary1_7)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims1_7 = get_averege_jaccard_sim(replacement_candidates1_7, minhash_dic1_7,get=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### 3737 vs 594 ################################\n",
      "jaccard similarity: 0.9310344827586207\n",
      "S0S3S3_1S3_2S3_3S5S5_1S5_1_2S4S4_2\n",
      "S0S3S3_1S3_2S3_3S5S5_1S5_1_2S4S4_3\n",
      "#######################################################################\n",
      "######################### 594 vs 3737 ################################\n",
      "jaccard similarity: 0.9310344827586207\n",
      "S0S3S3_1S3_2S3_3S5S5_1S5_1_2S4S4_3\n",
      "S0S3S3_1S3_2S3_3S5S5_1S5_1_2S4S4_2\n",
      "#######################################################################\n"
     ]
    }
   ],
   "source": [
    "if len(set(value for key,values in sims1_7.items() for value in values if value != 1.0)) != 0:\n",
    "    ans1_7 = min(set(value for key,values in sims1_7.items() for value in values if value != 1.0))\n",
    "    final_values = []\n",
    "    for key,values in sims1_7.items():\n",
    "        for value in values:\n",
    "            if value == ans1_7:\n",
    "                final_values.append(key)\n",
    "\n",
    "    dissimilar1_7 = set(final_values)\n",
    "    new_sims = []\n",
    "    for key in dissimilar1_7:\n",
    "        for value in replacement_candidates1_7[key]:\n",
    "            new_sims.append((key,value,jaccard_similarity(get_shingles(value,df_shingles1_7),get_shingles(key,df_shingles1_7))))\n",
    "    investigate1_7 = [case for case in new_sims if case[-1]!=1.0]\n",
    "    for case in investigate1_7:\n",
    "        print(f'######################### {case[0]} vs {case[1]} ################################')\n",
    "        print('jaccard similarity:',jaccard_similarity(get_shingles(case[0],df_shingles1_7), get_shingles(case[1],df_shingles1_7)))\n",
    "        print(get_traces(case[0],df_grouped1_7))\n",
    "        print(get_traces(case[1],df_grouped1_7))\n",
    "        print('#######################################################################')\n",
    "\n",
    "else:\n",
    "    print('all processes have approximate jaccard sim = 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(text, k=7):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text)-k +1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return list(set(shingle_set))\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_filtered2_1 = data.filter(data.type.isin(['Req']))\n",
    "df_grouped2_1 = df_filtered2_1.groupBy(\"user_id\").agg(concat_ws(\"\",collect_list(\"to\")).alias(\"features\"))\n",
    "\n",
    "shingles_udf2_1 = udf(shingle, ArrayType(StringType()))\n",
    "df_shingles2_1 = df_filtered2_1.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"trace\")) \\\n",
    "    .withColumn(\"shingles\", shingles_udf2_1(col(\"trace\"))) \\\n",
    "    .select(\"user_id\", \"shingles\")\n",
    "\n",
    "average_length2_1 = df_grouped2_1.select(avg(length(col('features')))).collect()[0][0]\n",
    "average_shingles2_1 = df_shingles2_1.withColumn(\"list_length\", size(col(\"shingles\"))) \\\n",
    "                     .agg(avg(\"list_length\").alias(\"average_list_length\")).collect()[0][0]\n",
    "\n",
    "print('average trace length:',average_length2_1)\n",
    "print('average trace # shingles:',average_shingles2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle2(text, k=3):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text)-k +1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return distinct_elements_in_order(shingle_set)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Filter Dataset\").getOrCreate()\n",
    "\n",
    "df_filtered2_2 = data.filter((col(\"from\") == \"S0\") & (~col(\"to\").contains(\"null\")) & (~col(\"to\").contains(\"_\")))\n",
    "df_grouped2_2 = df_filtered2_2.groupBy(\"user_id\").agg(concat_ws(\"\",collect_list(\"to\")).alias(\"features\"))\n",
    "\n",
    "# shingles_udata2_2 = udf(shingle2, ArrayType(StringType()))\n",
    "# df_shingles2_2 = df_filtered2_2.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"trace\")) \\\n",
    "#     .withColumn(\"shingles\", shingles_udata2_2(col(\"trace\"))) \\\n",
    "#     .select(\"user_id\", \"shingles\")\n",
    "\n",
    "# average_length2_2 = df_grouped2_2.select(avg(length(col('features')))).collect()[0][0]\n",
    "# average_shingles2_2 = df_shingles2_2.select(avg(size(col('shingles')))).collect()[0][0]\n",
    "\n",
    "# print('average trace length:',average_length2_2)\n",
    "# print('average trace # shingles:',average_shingles2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial number of cases: {df_grouped2_2.count()}\")\n",
    "ans = minhash_lsh(df_grouped2_2,3,0.98)\n",
    "replacement_candidates2_2, minhash_dic2_2 = ans[0],ans[1]\n",
    "new_process_dictionary2_2= bucketing(replacement_candidates2_2)\n",
    "print(f\"Number of unique processes after merging them with 0.97 threshold using 7-shingles: {len(new_process_dictionary2_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims2_2 = get_averege_jaccard_sim(new_process_dictionary2_2, minhash_dic2_2,get=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(set(value for key,values in sims2_2.items() for value in values if value != 1.0)) != 0:\n",
    "    ans = min(set(value for key,values in sims2_2.items() for value in values if value != 1.0))\n",
    "    final_values = []\n",
    "    for key,values in sims.items():\n",
    "        for value in values:\n",
    "            if value == ans:\n",
    "                final_values.append(key)\n",
    "\n",
    "    dissimilar2_2 = set(final_values)\n",
    "else:\n",
    "    print('all processes have approximate jaccard sim = 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfomance evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
