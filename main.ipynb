{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation import *\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,concat_ws, collect_list, lit,split, size, avg, udf, row_number, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "#from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "#from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy import average\n",
    "import numpy as np \n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "\n",
    "#os.environ[\"JAVA_HOME\"] = \"C:/Program Files/OpenJDK/jdk-21.0.2/bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "################################################# Question 1 ###################################################################\n",
    "################################################################################################################################\n",
    "def get_memory_usage(): \n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n",
    "\n",
    "def get_cpu_usage(): \n",
    "    return psutil.cpu_percent(interval=None)\n",
    "\n",
    "def get_performance(func1,func2, vals):\n",
    "    #k_values = [2, 3, 4, 5, 6, 7, 8]\n",
    "    results = []\n",
    "\n",
    "    for k in vals:\n",
    "        start_time = time.time()\n",
    "        start_mem = get_memory_usage()\n",
    "        start_cpu = get_cpu_usage()\n",
    "\n",
    "        replacement_candidates, minhashes = func1(df_grouped, k, 0.98)\n",
    "        new_process_dictionary = func2(replacement_candidates)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        end_mem = get_memory_usage()\n",
    "        end_cpu = get_cpu_usage()\n",
    "\n",
    "        duration = end_time - start_time\n",
    "        mem_used = end_mem - start_mem\n",
    "\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'time_seconds': duration,\n",
    "            'memory_mb': mem_used,\n",
    "            'unique_processes': len(new_process_dictionary),\n",
    "            'cpu': end_cpu\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def plot_results(results):\n",
    "    k_values = [result['k'] for result in results]\n",
    "    time_seconds = [result['time_seconds'] for result in results]\n",
    "    cpu_percentages = [result['cpu'] for result in results]\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax1.set_xlabel('k values')\n",
    "    ax1.set_ylabel('Time (seconds)', color='tab:blue')\n",
    "    ax1.plot(k_values, time_seconds, marker='o', color='tab:blue', label='Time')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('CPU Usage (%)', color='tab:red')\n",
    "\n",
    "    ax2.plot(k_values, cpu_percentages, marker='^', color='tab:red', linestyle='--', label='CPU Usage')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    plt.title('Performance Metrics for Different k Values')\n",
    "    fig.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_performances(results):\n",
    "    k_values = [result['k'] for result in results]\n",
    "    time_seconds = [result['time_seconds'] for result in results]\n",
    "    cpu_percentages = [result['cpu'] for result in results]\n",
    "\n",
    "    # Calculate the performance metric (Product of time_seconds and cpu)\n",
    "    performance_metric = [time_seconds[i] * cpu_percentages[i] for i in range(len(results))]\n",
    "\n",
    "    # Create a figure and axis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the performance metric\n",
    "    plt.plot(k_values, performance_metric, marker='o', linestyle='-', color='purple', label='Time * CPU')\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('k values')\n",
    "    plt.ylabel('Performance Metric (Time * CPU)')\n",
    "    plt.title('Combined Metric of Time and CPU Usage vs. k Values')\n",
    "    plt.xticks(k_values)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def shingle(text, k=7):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text)-k +1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return list(set(shingle_set))\n",
    "\n",
    "def jaccard_similarity(list1, list2):   \n",
    "    return len(set(list1).intersection(set(list2))) / len(set(list1).union(set(list2)))\n",
    "\n",
    "def minhash_lsh(df, k_shingle, threshold):\n",
    "\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
    "    minhashes = {}\n",
    "\n",
    "    for features in df.collect():\n",
    "        shingles = shingle(features[\"features\"], k_shingle)\n",
    "        m = MinHash(num_perm=128)\n",
    "        for shingle_item in shingles:\n",
    "            m.update(shingle_item.encode(\"utf8\"))\n",
    "        minhashes[int(features[\"user_id\"])] = m\n",
    "        lsh.insert(int(features[\"user_id\"]), m)\n",
    "\n",
    "    replacement_candidates = {}\n",
    "    for key in lsh.keys: \n",
    "        replacement_candidates[key] = lsh.query(minhashes[key]) \n",
    "\n",
    "    #Key: New representative, value: Similar items\n",
    "    return replacement_candidates,minhashes\n",
    "\n",
    "\n",
    "def remove_dups(graph):\n",
    "    keys_to_remove = []\n",
    "    for key, values in graph.items():\n",
    "        # Remove the key from its own value list if present\n",
    "        if key in values:\n",
    "            values.remove(key)\n",
    "        # Mark keys for removal if their value list is empty\n",
    "        if not values:\n",
    "            keys_to_remove.append(key)\n",
    "        else:\n",
    "            graph[key] = values  # Update the graph with the cleaned value list\n",
    "    \n",
    "    # Remove keys that had empty value lists\n",
    "    for key in keys_to_remove:\n",
    "        graph.pop(key)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def bucketing(replacement_candidates):\n",
    "    #replacement_candidates = remove_dups(replacement_candidates.copy())\n",
    "    visited_processes = set()\n",
    "    new_process_dictionary = {}\n",
    "\n",
    "    def bfs(start):\n",
    "        queue = [start]\n",
    "        bucket = []\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            if current not in visited_processes:\n",
    "                visited_processes.add(current)\n",
    "                bucket.append(current)\n",
    "                if current in replacement_candidates:\n",
    "                    for neighbor in replacement_candidates[current]:\n",
    "                        if neighbor not in visited_processes:\n",
    "                            queue.append(neighbor)\n",
    "        return bucket\n",
    "\n",
    "    for key in replacement_candidates.keys():\n",
    "        if key not in visited_processes:\n",
    "            bucket = bfs(key)\n",
    "            if bucket:\n",
    "                new_process_dictionary[key] = sorted(bucket)\n",
    "\n",
    "    return new_process_dictionary\n",
    "\n",
    "\n",
    "\n",
    "def get_case(caseID,data):\n",
    "    data1 = data.filter(data.user_id.isin([caseID]))\n",
    "    data1.show()\n",
    "\n",
    "def compare_cases(case1,case2,data):\n",
    "    data1 = data.filter(data.user_id.isin([case1]))\n",
    "    data2 = data.filter(data.user_id.isin([case2]))\n",
    "    desired_column_list1 = data1.select(\"to\").rdd.flatMap(lambda x: x).collect()\n",
    "    desired_column_list2 = data2.select(\"to\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    common_elements = np.intersect1d(desired_column_list1, desired_column_list2)\n",
    "    union_elements = np.union1d(desired_column_list1, desired_column_list2)\n",
    "    print(len(common_elements)/len(union_elements))\n",
    "\n",
    "def get_traces(user_id,df):\n",
    "    result = df.filter(col(\"user_id\") == user_id).select(\"features\").collect()\n",
    "    if result:\n",
    "        return result[0][\"features\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_shingles(user_id,df):\n",
    "    result = df.filter(col(\"user_id\") == user_id).select(\"shingles\").collect()\n",
    "    if result:\n",
    "        return result[0][\"shingles\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def write_df(df, file_name):\n",
    "    # Ensure the Output directory exists\n",
    "    os.makedirs('Output', exist_ok=True)\n",
    "    \n",
    "\n",
    "    columns = df.columns\n",
    "    columns.remove('user_id')\n",
    "    new_column_order = columns + ['user_id']\n",
    "    df = df.select(new_column_order)\n",
    "    \n",
    "    # Coalesce to a single partition and write the DataFrame to a text file\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv('temp/temp_output')\n",
    "    \n",
    "    # Find the part file and move it to the desired location\n",
    "    part_file = [f for f in os.listdir('temp/temp_output') if f.startswith(\"part-\")][0]\n",
    "    shutil.move(os.path.join('temp/temp_output', part_file), file_name)\n",
    "    shutil.rmtree('temp/temp_output')\n",
    "\n",
    "def output_part1(dataset, k, threshold,q=1):\n",
    "    spark = SparkSession.builder.appName(\"OutputUserIDs\").getOrCreate()\n",
    "    data = spark.read.csv(dataset, header=True, inferSchema=True)\n",
    "    df_filtered_m = data.filter(data.type.isin(['Req']))\n",
    "    df_grouped = df_filtered_m.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"features\"))\n",
    "    \n",
    "    replacement_candidates = minhash_lsh(df_grouped, k, threshold)\n",
    "    new_process_dictionary = bucketing(replacement_candidates[0])    \n",
    "\n",
    "    \n",
    "    user_ids_to_change = []#[key for key in new_process_dictionary.keys()]#[key for key, values in new_process_dictionary.items() if len(values) > 1 or ]\n",
    "    for key, values in new_process_dictionary.items():\n",
    "        if len(values) >1:\n",
    "            user_ids_to_change.append(key)\n",
    "        elif len(values) == 1 and values[0]!= key:\n",
    "            user_ids_to_change.append(key)\n",
    "\n",
    "    user_ids_to_delete = []\n",
    "    for id in user_ids_to_change:\n",
    "        for case in new_process_dictionary[id]:\n",
    "            if case not in user_ids_to_change:\n",
    "                user_ids_to_delete.append(case)\n",
    "\n",
    "    max_user_id = df_grouped.select(spark_max(\"user_id\")).collect()[0][0]\n",
    "\n",
    "    final_l = user_ids_to_change+user_ids_to_delete\n",
    "\n",
    "    df_to_change = df_grouped.filter(col(\"user_id\").isin(user_ids_to_change))\n",
    "    distinct_user_ids_to_change = df_to_change.select(\"user_id\").distinct()\n",
    "    window_spec = Window.orderBy(\"user_id\")\n",
    "    user_id_mapping = distinct_user_ids_to_change.withColumn(\"new_user_id\", row_number().over(window_spec) + max_user_id )\n",
    "    \n",
    "    df_with_new_ids = data.join(user_id_mapping, on=\"user_id\", how=\"left\")\n",
    "    df_with_new_ids = df_with_new_ids.withColumn(\"user_id\",\n",
    "                                                 when(col(\"new_user_id\").isNotNull(), col(\"new_user_id\"))\n",
    "                                                 .otherwise(col(\"user_id\"))) \\\n",
    "                                     .drop(\"new_user_id\")\n",
    "\n",
    "    columns = df_with_new_ids.columns\n",
    "    columns.remove('user_id')\n",
    "    new_column_order = columns + ['user_id']\n",
    "    df_with_new_ids = df_with_new_ids.select(new_column_order)\n",
    "    \n",
    "    return df_with_new_ids, user_ids_to_delete, user_ids_to_change\n",
    "\n",
    "\n",
    "################################################################################################################################\n",
    "################################################# Question 2 ###################################################################\n",
    "################################################################################################################################\n",
    "\n",
    "def kmeans_clustering(df, n_clusters, max_iter):\n",
    "    minhashes = []\n",
    "    #for jaccard verification\n",
    "    minhash_dict = {}\n",
    "    user_ids = []\n",
    "    final_buckets = {}\n",
    "    for features in df.collect():\n",
    "        shingles = shingle(features[\"features\"], 5)\n",
    "        m = MinHash(num_perm=128)\n",
    "        for shingle_item in shingles:\n",
    "            m.update(shingle_item.encode(\"utf8\"))\n",
    "        minhashes.append(m.hashvalues)\n",
    "        minhash_dict[int(features[\"user_id\"])] = m\n",
    "        user_ids.append(int(features[\"user_id\"]))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, max_iter=max_iter).fit(minhashes)\n",
    "\n",
    "    user_clusters = dict(zip(user_ids, kmeans.labels_))\n",
    "    final_buckets = {}\n",
    "    for key, value in user_clusters.items():\n",
    "        if value in final_buckets:\n",
    "            final_buckets[value].append(key)\n",
    "        else:\n",
    "            final_buckets[value] = [key]\n",
    "\n",
    "    return final_buckets, minhash_dict\n",
    "\n",
    "def get_averege_jaccard_sim(final_buckets, minhashes,get = True):\n",
    "    sims = {}\n",
    "    for key, value in final_buckets.items():\n",
    "        for user_id_1 in final_buckets[key]:\n",
    "            for user_id_2 in final_buckets[key]:\n",
    "                if user_id_1 != user_id_2:\n",
    "                    sig_1 = minhashes[int(user_id_1)]\n",
    "                    sig_2 = minhashes[int(user_id_2)]\n",
    "                    sim = MinHash.jaccard(sig_1, sig_2)\n",
    "                    if key not in sims:\n",
    "                        sims[key] = [sim]\n",
    "                    else:\n",
    "                        sims[key].append(sim)\n",
    "    total_sum = 0\n",
    "    total_count = 0\n",
    "    sims = dict(sorted(sims.items()))\n",
    "    if get == True:\n",
    "        for key, value in sims.items():\n",
    "            avg_sim = average(value)\n",
    "            print(key, avg_sim)\n",
    "            total_sum += sum(value)\n",
    "            total_count += len(value)\n",
    "        \n",
    "        overall_average = total_sum / total_count if total_count != 0 else 0\n",
    "        print(\"Overall Average Jaccard Similarity:\", overall_average)\n",
    "\n",
    "    return sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(text, k=7):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text)-k +1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return list(set(shingle_set))\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\"data/SDG_dataset1.csv\", header=True, inferSchema=True)\n",
    "df_filtered_m = data.filter(data.type.isin(['Req']))\n",
    "df_grouped = df_filtered_m.groupBy(\"user_id\").agg(concat_ws(\"\",collect_list(\"to\")).alias(\"features\"))\n",
    "\n",
    "shingles_udf = udf(shingle, ArrayType(StringType()))\n",
    "df_shingles = df_filtered_m.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"trace\")) \\\n",
    "    .withColumn(\"shingles\", shingles_udf(col(\"trace\"))) \\\n",
    "    .select(\"user_id\", \"shingles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S0S1S1_3S2S2_4S3S3_1S3_2S3_3S4S4_2S5'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_traces(12,df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data2 = spark.read.csv(\"Output/part1Output.txt\", header=True, inferSchema=True)\n",
    "df_filtered_m2 = data2.filter(data2.type.isin(['Req']))\n",
    "df_grouped2 = df_filtered_m2.groupBy(\"user_id\").agg(concat_ws(\"\",collect_list(\"to\")).alias(\"features\"))\n",
    "\n",
    "shingles_udf = udf(shingle, ArrayType(StringType()))\n",
    "df_shingles2 = df_filtered_m2.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"trace\")) \\\n",
    "    .withColumn(\"shingles\", shingles_udf(col(\"trace\"))) \\\n",
    "    .select(\"user_id\", \"shingles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0S1S1_2S2S2_2S2_3S2_4S3S3_1S3_2S3_3S4S4_1S5\n",
      "S0S1S1_2S2S2_2S2_3S2_4S3S3_1S3_2S3_3S4S4_2S5\n",
      "S0S1S1_2S2S2_2S2_3S2_4S3S3_1S3_2S3_3S4S4_2S5\n"
     ]
    }
   ],
   "source": [
    "print(get_traces(9,df_grouped))\n",
    "print(get_traces(5,df_grouped))\n",
    "print(get_traces(37,df_grouped2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no dups\n",
      "{31: [31], 34: [34], 28: [28], 26: [26], 27: [27], 12: [12], 22: [22], 1: [1], 13: [13], 6: [6], 16: [16, 33], 3: [3], 20: [20], 40: [40], 5: [5], 19: [19], 41: [41], 15: [15], 43: [43], 37: [37], 9: [9], 17: [17], 35: [35], 4: [4], 8: [8], 23: [23], 39: [39], 7: [7], 10: [10], 38: [38], 25: [25], 24: [24], 29: [29], 21: [21], 32: [32], 11: [11], 33: [16, 33], 14: [14], 42: [42], 2: [2], 30: [30], 18: [18], 36: [36]}\n",
      "final\n",
      "{}\n",
      "minhash\n",
      "({31: [31], 34: [34], 28: [28], 26: [26], 27: [27], 12: [12], 22: [22], 1: [1], 13: [13], 6: [6], 16: [16, 33], 3: [3], 20: [20], 40: [40], 5: [5], 19: [19], 41: [41], 15: [15], 43: [43], 37: [37], 9: [9], 17: [17], 35: [35], 4: [4], 8: [8], 23: [23], 39: [39], 7: [7], 10: [10], 38: [38], 25: [25], 24: [24], 29: [29], 21: [21], 32: [32], 11: [11], 33: [33], 14: [14], 42: [42], 2: [2], 30: [30], 18: [18], 36: [36]}, {31: <datasketch.minhash.MinHash object at 0x0000021E086DD570>, 34: <datasketch.minhash.MinHash object at 0x0000021E0750B0A0>, 28: <datasketch.minhash.MinHash object at 0x0000021E086DCE50>, 26: <datasketch.minhash.MinHash object at 0x0000021E086DD5D0>, 27: <datasketch.minhash.MinHash object at 0x0000021E07DF3730>, 12: <datasketch.minhash.MinHash object at 0x0000021E07DF1900>, 22: <datasketch.minhash.MinHash object at 0x0000021E07DF0610>, 1: <datasketch.minhash.MinHash object at 0x0000021E07DF2710>, 13: <datasketch.minhash.MinHash object at 0x0000021E07DF2200>, 6: <datasketch.minhash.MinHash object at 0x0000021E07DF3490>, 16: <datasketch.minhash.MinHash object at 0x0000021E07DF1ED0>, 3: <datasketch.minhash.MinHash object at 0x0000021E07DF1EA0>, 20: <datasketch.minhash.MinHash object at 0x0000021E07DF1D50>, 40: <datasketch.minhash.MinHash object at 0x0000021E087E5750>, 5: <datasketch.minhash.MinHash object at 0x0000021E087E5630>, 19: <datasketch.minhash.MinHash object at 0x0000021E087E6440>, 41: <datasketch.minhash.MinHash object at 0x0000021E087E5000>, 15: <datasketch.minhash.MinHash object at 0x0000021E08E0F130>, 43: <datasketch.minhash.MinHash object at 0x0000021E06CB1510>, 37: <datasketch.minhash.MinHash object at 0x0000021E06CB19F0>, 9: <datasketch.minhash.MinHash object at 0x0000021E08E0F580>, 17: <datasketch.minhash.MinHash object at 0x0000021E07DCB2B0>, 35: <datasketch.minhash.MinHash object at 0x0000021E07DCB520>, 4: <datasketch.minhash.MinHash object at 0x0000021E07DCB160>, 8: <datasketch.minhash.MinHash object at 0x0000021E07DC9E10>, 23: <datasketch.minhash.MinHash object at 0x0000021E07DCBF40>, 39: <datasketch.minhash.MinHash object at 0x0000021E07DCB0D0>, 7: <datasketch.minhash.MinHash object at 0x0000021E07DCBFD0>, 10: <datasketch.minhash.MinHash object at 0x0000021E07DCB9D0>, 38: <datasketch.minhash.MinHash object at 0x0000021E07DCA0B0>, 25: <datasketch.minhash.MinHash object at 0x0000021E07DCA320>, 24: <datasketch.minhash.MinHash object at 0x0000021E07DCB310>, 29: <datasketch.minhash.MinHash object at 0x0000021E07DCA6B0>, 21: <datasketch.minhash.MinHash object at 0x0000021E07DCA080>, 32: <datasketch.minhash.MinHash object at 0x0000021E07DCBF70>, 11: <datasketch.minhash.MinHash object at 0x0000021E07DCB940>, 33: <datasketch.minhash.MinHash object at 0x0000021E07DC9F90>, 14: <datasketch.minhash.MinHash object at 0x0000021E07DC9FC0>, 42: <datasketch.minhash.MinHash object at 0x0000021E07DCBC40>, 2: <datasketch.minhash.MinHash object at 0x0000021E07DCBA30>, 30: <datasketch.minhash.MinHash object at 0x0000021E07DCBAF0>, 18: <datasketch.minhash.MinHash object at 0x0000021E07DCA3E0>, 36: <datasketch.minhash.MinHash object at 0x0000021E07DCBD90>})\n",
      "buckets\n",
      "{}\n",
      "to change\n",
      "[]\n",
      "to delete\n",
      "[]\n",
      "43\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def remove_node_from_neighbors(graph, node_to_remove):\n",
    "    if node_to_remove in graph:\n",
    "        # Remove node_to_remove from all adjacency lists in the graph\n",
    "        for node, neighbors in graph.items():\n",
    "            if node != node_to_remove:  # Skip the node_to_remove itself\n",
    "                if node_to_remove in neighbors:\n",
    "                    neighbors.remove(node_to_remove)\n",
    "\n",
    "def dfs(graph, start, visited):\n",
    "    stack = [start]\n",
    "    result = []\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            result.append(node)\n",
    "            stack.extend(graph[node])\n",
    "    return result\n",
    "\n",
    "def transform_graph(graph):\n",
    "    reachable_dict = {}\n",
    "    for node in graph:\n",
    "        visited = set()\n",
    "        reachable_nodes = dfs(graph, node, visited)\n",
    "        reachable_dict[node] = [n for n in reachable_nodes if n != node]  # Exclude the start node itself\n",
    "    \n",
    "    # Filter out keys that also appear as values\n",
    "    keys_to_remove = set()\n",
    "    for node, neighbors in graph.items():\n",
    "        keys_to_remove.update(neighbors)\n",
    "    \n",
    "    final_dict = {k: v for k, v in reachable_dict.items() if k not in keys_to_remove}\n",
    "    return final_dict\n",
    "\n",
    "def bucketing(old_replacement_candidates):\n",
    "    alt_old_replacement_candidates = old_replacement_candidates.copy()\n",
    "    replacement_candidates = alt_old_replacement_candidates\n",
    "    print('no dups')\n",
    "    print(replacement_candidates)\n",
    "    visited = set()\n",
    "    for node in replacement_candidates.keys():\n",
    "        if node not in visited:\n",
    "            remove_node_from_neighbors(replacement_candidates, node)\n",
    "        for neighbour in replacement_candidates[node]:\n",
    "            visited.add(neighbour)\n",
    "    # print('no neighbours')\n",
    "    # print(replacement_candidates)\n",
    "    new_process_dictionary = transform_graph(replacement_candidates)\n",
    "    print('final')\n",
    "    print(new_process_dictionary)\n",
    "    \n",
    "        \n",
    "    return new_process_dictionary\n",
    "\n",
    "#generate_dataset(tasks, 100000,start_time,end_time,file_name=\"dataset1\",random=False,connect=False)  \n",
    "#print('minhash')\n",
    "ans = output_part1(\"data/SDG_dataset1.csv\", 5, 0.97)\n",
    "#print(ans[39])\n",
    "print(ans)\n",
    "#ans2 = bucketing(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ans[34]\n",
    "print(test)\n",
    "for value in test:\n",
    "    new_values = ans[value]\n",
    "    for new_value in new_values:\n",
    "        if new_value not in ans2[34]:\n",
    "            print('False')\n",
    "            print(new_value)\n",
    "print(ans2[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "output_part1(\"data/SDG_dataset1.csv\",7,0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\"data/SDG_dataset2.csv\", header=True, inferSchema=True)\n",
    "df_filtered_m = data.filter(data.type.isin(['Req']))\n",
    "df_grouped = df_filtered_m.groupBy(\"user_id\").agg(concat_ws(\"\",collect_list(\"to\")).alias(\"features\"))\n",
    "\n",
    "shingles_udf = udf(shingle, ArrayType(StringType()))\n",
    "df_shingles = df_filtered_m.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"trace\")) \\\n",
    "    .withColumn(\"shingles\", shingles_udf(col(\"trace\"))) \\\n",
    "    .select(\"user_id\", \"shingles\")\n",
    "\n",
    "#data.show()\n",
    "#df_filtered_m.show()\n",
    "#df_grouped.show()\n",
    "#df_shingles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramter-tuning for k-shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to see which k is the best for the k-shingles. We gonna do such a thing by investigating how computational expensive it is to compute such minhash for different values of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_performance(minhash_lsh,bucketing, [3,4,5,6,7,8,9])\n",
    "plot_results(results)\n",
    "plot_performances(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running this a considerable amount of times we saw that the value that get a better balance between time and CPU usage is when k=7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter-tuning for the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_num_shingles = df_shingles.withColumn(\"list_length\", size(col(\"shingles\"))) \\\n",
    "                     .agg(avg(\"list_length\").alias(\"average_list_length\")).collect()[0][0]\n",
    "average_num_shingles\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the average number of 7-shingles is 32, and we want to group processes with only small variations, we want that 31/32 shingles to be the same, so that we still allow for some small variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial number of cases: {df_grouped.count()}\")\n",
    "ans = minhash_lsh(df_grouped,7,0.97)\n",
    "replacement_candidates7, minhash_dic = ans[0],ans[1]\n",
    "new_process_dictionary7= bucketing(replacement_candidates7)\n",
    "print(f\"Number of unique processes after merging them with 0.97 threshold using 7-shingles: {len(new_process_dictionary7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the processes with a threshold of 97%, using 7-shingles, we obtain 29729 candidate unique cases. In order to investigate if further analysis into the similarities needs to be done, to make sure that the false positives do not result in cases where the cases are not small variations of each other, we are going to compute the average similarities of all buckets and investigate the mininum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = get_averege_jaccard_sim(replacement_candidates7, minhash_dic,get=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to mention that we're still just computing the approximate jaccard similarities provided by MinHashLSH, so, in order to investigate the cases that have the smallest approximate jaccard similarities, we're going to compute the actual similarities between those cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = min(set(value for key,values in sims.items() for value in values if value != 1.0))\n",
    "final_values = []\n",
    "for key,values in sims.items():\n",
    "    for value in values:\n",
    "        if value == ans:\n",
    "            final_values.append(key)\n",
    "\n",
    "dissimilar = set(final_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sims = []\n",
    "for key in dissimilar:\n",
    "    for value in replacement_candidates7[key]:\n",
    "        new_sims.append((key,value,jaccard_similarity(get_shingles(value),get_shingles(key))))\n",
    "investigate = [case for case in new_sims if case[-1]!=1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in investigate:\n",
    "    print(f'######################### {case[0]} vs {case[1]} ################################')\n",
    "    print(get_traces(case[0],df_grouped))\n",
    "    print(get_traces(case[1],df_grouped))\n",
    "    print('#######################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_part1(\"data/SDG_dataset2.csv\",3,0.95)\n",
    "# output_part1(\"data/SDG_dataset2.csv\",5,0.95)\n",
    "output_part1(\"data/SDG_dataset2.csv\",7,0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning for Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "minhashes = []\n",
    "user_ids = []\n",
    "final_buckets = {}\n",
    "for features in df_grouped.collect():\n",
    "    shingles = shingle(features[\"features\"], 5)\n",
    "    m = MinHash(num_perm=128)\n",
    "    for shingle_item in shingles:\n",
    "        m.update(shingle_item.encode(\"utf8\"))\n",
    "    minhashes.append(m.hashvalues)\n",
    "    user_ids.append(int(features[\"user_id\"]))\n",
    "\n",
    "param_grid = {\n",
    "    'n_clusters': [100, 250, 500],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "}\n",
    "\n",
    "kmeans = KMeans()\n",
    "\n",
    "grid_search = GridSearchCV(kmeans, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(minhashes)\n",
    "\n",
    "best_param = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"The best parameters are: \" , best_param )\n",
    "print(\"The best model is: \", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Find and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_candidates = minhash_lsh(df_grouped,5,0.7)\n",
    "new_process_dictionary = bucketing(replacement_candidates)\n",
    "print(len(replacement_candidates))\n",
    "print(len(new_process_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Find/cluster similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "users = []\n",
    "for key in new_process_dictionary:\n",
    "    users.append(key)\n",
    "\n",
    "filtered_df = df_grouped[df_grouped['user_id'].isin(users)]\n",
    "\n",
    "final_buckets, minhashes = kmeans_clustering(filtered_df,500,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_averege_jaccard_sim(final_buckets, minhashes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Initial number of cases: {df_grouped.count()}\")\n",
    "# replacement_candidates3 = minhash_lsh(df_grouped,3,0.98)\n",
    "# new_process_dictionary3 = bucketing(replacement_candidates3)\n",
    "#print(f\"After merging cases with threshold 3-shingles: {len(new_process_dictionary3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split = df_grouped.withColumn(\"feature_length\", size(split(col(\"features\"), \"S\")))\n",
    "# average_length = df_split.agg(avg(\"feature_length\")).collect()[0][0]\n",
    "\n",
    "# windowSpec = Window.partitionBy(F.lit(1)).orderBy(\"feature_length\")\n",
    "# df_split = df_split.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "# total_count = df_split.count()\n",
    "\n",
    "# if total_count % 2 == 0:\n",
    "#     median_index1 = total_count // 2\n",
    "#     median_index2 = median_index1 + 1\n",
    "#     median_value = df_split.filter(col(\"row_number\").isin([median_index1, median_index2])) \\\n",
    "#                                   .agg(avg(\"feature_length\")).collect()[0][0]\n",
    "# else:\n",
    "#     median_index = (total_count // 2) + 1\n",
    "#     median_value = df_split.filter(col(\"row_number\") == median_index) \\\n",
    "#                                   .select(\"feature_length\").collect()[0][0]\n",
    "    \n",
    "# print(f\"Average number of requests: {average_length}\")\n",
    "# print(f\"Median number of requests: {median_value}\")\n",
    "\n",
    "\n",
    "#Given that both the average and the median are close to 13, which shows that the dataset is symetricly distributed when it comes to how many \n",
    "#requests are performed, we gonna assume that two cases have a small variation iif the number of different requests is around 1. To get an \n",
    "# #approximation of the threshold, we're going to use the resutls shown before, so we assume that 12/13 are the same. Given this, we decided \n",
    "# to use a threshold of 95%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
