{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Word2Vec, StringIndexer, VectorAssembler, DataFrame\n",
    "from pyspark.sql.functions import col, array, explode, concat_ws, collect_list, udf, lit\n",
    "import numpy as np \n",
    "from pyspark.sql.types import StringType, ArrayType, BooleanType\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy import average\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(text, k):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text)-k +1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return set(shingle_set)\n",
    "\n",
    "def minhash_lsh(df, k_shingle, threshold):\n",
    "\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
    "    minhashes = {}\n",
    "\n",
    "    for features in df.collect():\n",
    "        shingles = shingle(features[\"features\"], k_shingle)\n",
    "        m = MinHash(num_perm=128)\n",
    "        for shingle_item in shingles:\n",
    "            m.update(shingle_item.encode(\"utf8\"))\n",
    "        minhashes[int(features[\"user_id\"])] = m\n",
    "        lsh.insert(int(features[\"user_id\"]), m)\n",
    "\n",
    "    replacement_candidates = {}\n",
    "    for key in lsh.keys: \n",
    "        replacement_candidates[key] = lsh.query(minhashes[key]) \n",
    "\n",
    "    #Key: New representative, value: Similar items\n",
    "    return replacement_candidates\n",
    "\n",
    "#Iteratively bucket unique processes together\n",
    "def bucketing(replacement_candidates):\n",
    "    visited_processes = set()\n",
    "    new_process_dictionary = {}\n",
    "    for key, values in replacement_candidates.items():\n",
    "        new_values = []\n",
    "        for value in values:\n",
    "            if value not in visited_processes:\n",
    "                visited_processes.add(value)\n",
    "                new_values.append(value)\n",
    "        if new_values:  # Only add non-empty lists\n",
    "            new_process_dictionary[key] = sorted(new_values)\n",
    "    return new_process_dictionary\n",
    "\n",
    "\n",
    "def kmeans_clustering(df, n_clusters, max_iter):\n",
    "    minhashes = []\n",
    "    #for jaccard verification\n",
    "    minhash_dict = {}\n",
    "    user_ids = []\n",
    "    final_buckets = {}\n",
    "    for features in df.collect():\n",
    "        shingles = shingle(features[\"features\"], 5)\n",
    "        m = MinHash(num_perm=128)\n",
    "        for shingle_item in shingles:\n",
    "            m.update(shingle_item.encode(\"utf8\"))\n",
    "        minhashes.append(m.hashvalues)\n",
    "        minhash_dict[int(features[\"user_id\"])] = m\n",
    "        user_ids.append(int(features[\"user_id\"]))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, max_iter=max_iter).fit(minhashes)\n",
    "\n",
    "    user_clusters = dict(zip(user_ids, kmeans.labels_))\n",
    "    final_buckets = {}\n",
    "    for key, value in user_clusters.items():\n",
    "        if value in final_buckets:\n",
    "            final_buckets[value].append(key)\n",
    "        else:\n",
    "            final_buckets[value] = [key]\n",
    "\n",
    "    return final_buckets, minhash_dict\n",
    "\n",
    "\n",
    "#Get averege jaccard value per bucket\n",
    "\n",
    "def get_averege_jaccard_sim(final_buckets, minhashes):\n",
    "    sims = {}\n",
    "    for key, value in final_buckets.items():\n",
    "        for user_id_1 in final_buckets[key]:\n",
    "            for user_id_2 in final_buckets[key]:\n",
    "                if user_id_1 != user_id_2:\n",
    "                    sig_1 = minhashes[int(user_id_1)]\n",
    "                    sig_2 = minhashes[int(user_id_2)]\n",
    "                    sim = MinHash.jaccard(sig_1, sig_2)\n",
    "                    if key not in sims:\n",
    "                        sims[key] = [sim]\n",
    "                    else:\n",
    "                        sims[key].append(sim)\n",
    "    total_sum = 0\n",
    "    total_count = 0\n",
    "    sims = dict(sorted(sims.items()))\n",
    "    for key, value in sims.items():\n",
    "        avg_sim = average(value)\n",
    "        print(key, avg_sim)\n",
    "        total_sum += sum(value)\n",
    "        total_count += len(value)\n",
    "    \n",
    "    overall_average = total_sum / total_count if total_count != 0 else 0\n",
    "    print(\"Overall Average Jaccard Similarity:\", overall_average)\n",
    "\n",
    "\n",
    "def write_df(df,file_name):\n",
    "    os.makedirs('Output', exist_ok=True)\n",
    "    #temporary folder to save all the temporaty files created by write.csv\n",
    "    os.makedirs('temp', exist_ok=True)\n",
    "    df.write.csv('temp/temp_outoput', header=True, mode=\"overwrite\")\n",
    "    part_file = [f for f in os.listdir('temp/temp_outoput') if f.startswith(\"part-\")][0]\n",
    "\n",
    "    shutil.move(os.path.join('temp/temp_outoput', part_file), file_name)\n",
    "    shutil.rmtree('temp/temp_outoput')\n",
    "    shutil.rmtree('temp')\n",
    "\n",
    "def output_part1(dataset,k,threshold):\n",
    "    data = spark.read.csv(dataset, header=True, inferSchema=True)\n",
    "    df = data.withColumn(\"arrayColumn\", concat_ws(\"\",\"from\", \"to\")).withColumn(\"Minhash\", lit(\"\"))\n",
    "    df_grouped = df.groupBy(\"user_id\").agg(\n",
    "                                            concat_ws(\"\",collect_list(\"arrayColumn\")).alias(\"features\"))\n",
    "    \n",
    "    replacement_candidates = minhash_lsh(df_grouped,k,threshold)\n",
    "    new_process_dictionary = bucketing(replacement_candidates)    \n",
    "\n",
    "    user_ids = list(new_process_dictionary.keys())\n",
    "    output_df = data.filter(df.user_id.isin(user_ids))\n",
    "    final_df = output_df.coalesce(1)\n",
    "    return write_df(final_df,'Output/part1Output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"spark_session_1\").getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\"data/SDG_dataset2.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df = data.withColumn(\"arrayColumn\", concat_ws(\"\",\"from\", \"to\")).withColumn(\"Minhash\", lit(\"\"))\n",
    "\n",
    "df_grouped = df.groupBy(\"user_id\").agg(\n",
    "    concat_ws(\"\",collect_list(\"arrayColumn\")).alias(\"features\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter-tuning for k-shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial processes: 45641\n",
      "After merging processes: 8295\n",
      "Initial processes: 45641\n",
      "After merging processes: 12488\n",
      "Initial processes: 45641\n",
      "After merging processes: 33645\n"
     ]
    }
   ],
   "source": [
    "replacement_candidates = minhash_lsh(df_grouped,3, 0.95)\n",
    "new_process_dictionary = bucketing(replacement_candidates)\n",
    "print(f\"Initial processes: {len(replacement_candidates)}\")\n",
    "print(f\"After merging processes: {len(new_process_dictionary)}\")\n",
    "\n",
    "replacement_candidates = minhash_lsh(df_grouped,5,0.95)\n",
    "new_process_dictionary = bucketing(replacement_candidates)\n",
    "print(f\"Initial processes: {len(replacement_candidates)}\")\n",
    "print(f\"After merging processes: {len(new_process_dictionary)}\")\n",
    "\n",
    "replacement_candidates = minhash_lsh(df_grouped,7, 0.95)\n",
    "new_process_dictionary = bucketing(replacement_candidates)\n",
    "print(f\"Initial processes: {len(replacement_candidates)}\")\n",
    "print(f\"After merging processes: {len(new_process_dictionary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning for Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:  {'max_iter': 100, 'n_clusters': 500}\n",
      "The best model is:  KMeans(max_iter=100, n_clusters=500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "minhashes = []\n",
    "user_ids = []\n",
    "final_buckets = {}\n",
    "for features in df_grouped.collect():\n",
    "    shingles = shingle(features[\"features\"], 5)\n",
    "    m = MinHash(num_perm=128)\n",
    "    for shingle_item in shingles:\n",
    "        m.update(shingle_item.encode(\"utf8\"))\n",
    "    minhashes.append(m.hashvalues)\n",
    "    user_ids.append(int(features[\"user_id\"]))\n",
    "\n",
    "param_grid = {\n",
    "    'n_clusters': [100, 250, 500, 1000],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "}\n",
    "\n",
    "kmeans = KMeans()\n",
    "\n",
    "grid_search = GridSearchCV(kmeans, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(minhashes)\n",
    "\n",
    "best_param = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"The best parameters are: \" , best_param )\n",
    "print(\"The best model is: \", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Find and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45641\n",
      "33645\n"
     ]
    }
   ],
   "source": [
    "replacement_candidates = minhash_lsh(df_grouped,7, 0.95)\n",
    "new_process_dictionary = bucketing(replacement_candidates)\n",
    "print(len(replacement_candidates))\n",
    "print(len(new_process_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o33858.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m write_df(final_df,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput/part1Output.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# output_part1(\"data/SDG_dataset2.csv\",3,0.95)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# output_part1(\"data/SDG_dataset2.csv\",5,0.95)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43moutput_part1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/SDG_dataset2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 27\u001b[0m, in \u001b[0;36moutput_part1\u001b[1;34m(dataset, k, threshold)\u001b[0m\n\u001b[0;32m     25\u001b[0m output_df \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfilter(df\u001b[38;5;241m.\u001b[39muser_id\u001b[38;5;241m.\u001b[39misin(user_ids))\n\u001b[0;32m     26\u001b[0m final_df \u001b[38;5;241m=\u001b[39m output_df\u001b[38;5;241m.\u001b[39mcoalesce(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrite_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_df\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOutput/part1Output.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m, in \u001b[0;36mwrite_df\u001b[1;34m(df, file_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#temporary folder to save all the temporaty files created by write.csv\u001b[39;00m\n\u001b[0;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemp/temp_outoput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m part_file \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp/temp_outoput\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart-\u001b[39m\u001b[38;5;124m\"\u001b[39m)][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m shutil\u001b[38;5;241m.\u001b[39mmove(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp/temp_outoput\u001b[39m\u001b[38;5;124m'\u001b[39m, part_file), file_name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o33858.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "output_part1(\"data/SDG_dataset2.csv\",7,0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Find/cluster similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "users = []\n",
    "for key in new_process_dictionary:\n",
    "    users.append(key)\n",
    "\n",
    "filtered_df = df_grouped[df_grouped['user_id'].isin(users)]\n",
    "\n",
    "final_buckets, minhashes = kmeans_clustering(filtered_df,500,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7834201388888888\n",
      "1 0.847677517961901\n",
      "2 0.7447797619047619\n",
      "3 0.7051776208026208\n",
      "4 0.8617927376229084\n",
      "5 0.7972745028409091\n",
      "6 0.7836498001262361\n",
      "7 0.9415954415954416\n",
      "8 0.8053662455673759\n",
      "9 0.8356819600499376\n",
      "10 0.8339362026862027\n",
      "11 0.7604993386243386\n",
      "12 0.8120081018518519\n",
      "13 0.723417467948718\n",
      "14 1.0\n",
      "15 0.8237791719948849\n",
      "16 0.8545228687739463\n",
      "17 0.8591193990042674\n",
      "18 0.8710798012730943\n",
      "19 0.7525192840189874\n",
      "20 0.8004079693532818\n",
      "21 0.7447916666666666\n",
      "22 0.8426411290322581\n",
      "23 0.8158482142857143\n",
      "24 0.8285163849708171\n",
      "25 0.8749687208277149\n",
      "26 0.7049501050420168\n",
      "27 0.8059927208835341\n",
      "28 0.759171195652174\n",
      "29 0.7497585338419406\n",
      "30 0.8264365290034933\n",
      "31 0.9097732843137255\n",
      "32 0.8368202110389611\n",
      "33 0.8504585597826086\n",
      "34 0.801110197368421\n",
      "35 0.8607062177280551\n",
      "36 0.7909757653061225\n",
      "37 0.8297011066763426\n",
      "38 0.8109365816926242\n",
      "39 0.7466061282467532\n",
      "40 0.852827380952381\n",
      "41 0.8078188259109311\n",
      "42 0.7372295673076923\n",
      "43 0.846875\n",
      "44 0.8608469202898551\n",
      "45 0.8153409090909091\n",
      "46 0.8214492394179894\n",
      "47 0.7838322506837787\n",
      "48 0.810264423076923\n",
      "49 0.8489935247747747\n",
      "50 0.8027548480083857\n",
      "51 0.887328506097561\n",
      "52 0.8408695539657711\n",
      "53 0.776171875\n",
      "54 0.8855255516840883\n",
      "55 0.7936256451881452\n",
      "56 0.686298487696514\n",
      "57 0.7075279768577835\n",
      "58 0.8653002084292364\n",
      "59 0.7950431034482759\n",
      "60 0.6796656621243885\n",
      "61 0.8189467656379168\n",
      "62 0.7398886494252873\n",
      "63 0.7964728860294118\n",
      "64 0.8824372552051124\n",
      "65 0.85625\n",
      "66 0.7873076265389877\n",
      "67 0.8899671712109062\n",
      "68 0.8489583333333334\n",
      "69 0.9188506036217304\n",
      "70 0.8108583664349553\n",
      "71 0.849346080955446\n",
      "72 0.8402534298780487\n",
      "73 0.699781680764411\n",
      "74 0.7755070612980769\n",
      "75 0.8694444444444445\n",
      "76 0.8799534386841893\n",
      "77 0.8005288461538461\n",
      "78 0.7843406593406593\n",
      "79 0.8994708994708994\n",
      "80 0.7969401041666667\n",
      "81 0.8582682291666667\n",
      "82 0.7621410472972973\n",
      "83 0.6982040229885057\n",
      "84 0.8138706140350878\n",
      "85 0.773739067413522\n",
      "86 0.7996572293447294\n",
      "87 0.8222950268817204\n",
      "88 0.790330615942029\n",
      "89 0.8628798558897243\n",
      "90 0.8228241604477612\n",
      "91 0.8326388888888889\n",
      "92 0.7834547776442308\n",
      "93 0.9025409992372235\n",
      "94 0.7214066772043746\n",
      "95 0.8439327485380117\n",
      "96 0.8602039300568712\n",
      "97 0.8438335842810115\n",
      "98 0.6831951530612245\n",
      "99 0.9289423076923077\n",
      "100 0.8274617346938775\n",
      "101 0.7657054375804376\n",
      "102 0.760912230458221\n",
      "103 0.8558124011075949\n",
      "104 0.8428933262917638\n",
      "105 0.8830221036585366\n",
      "106 0.8229417281508412\n",
      "107 0.7922054597701149\n",
      "108 0.8747876490486678\n",
      "109 0.7377878289473684\n",
      "110 0.8043380939902166\n",
      "111 0.8876767309725159\n",
      "112 0.7295160455486542\n",
      "113 0.85078125\n",
      "114 0.823397575493612\n",
      "115 0.8263325216450217\n",
      "116 0.885440340909091\n",
      "117 0.7875434027777778\n",
      "118 0.8384725595308613\n",
      "119 0.8029891304347826\n",
      "120 0.8619351112155389\n",
      "121 0.885084219858156\n",
      "122 0.8283547794117647\n",
      "123 0.8636769141184902\n",
      "124 0.8739476768968457\n",
      "125 0.806046195652174\n",
      "126 0.8386703431372549\n",
      "127 0.7448958333333333\n",
      "128 0.802448745834876\n",
      "129 0.7768549280177187\n",
      "130 0.896484375\n",
      "131 0.8190617721273292\n",
      "132 0.9598214285714286\n",
      "133 0.8636850319396051\n",
      "134 0.8462372448979592\n",
      "135 0.7837180603948897\n",
      "136 0.8403795249448803\n",
      "137 0.8587190823576727\n",
      "138 0.7942708333333334\n",
      "139 0.8266369047619048\n",
      "140 0.7970383121468927\n",
      "141 0.8438858695652174\n",
      "142 0.7685661643610013\n",
      "143 0.8586950231481482\n",
      "144 0.8025240384615384\n",
      "145 0.8159326485624502\n",
      "146 0.8905066287878788\n",
      "147 0.7966517857142857\n",
      "148 0.8690439529675252\n",
      "149 0.7611166007905138\n",
      "150 0.8404568068747174\n",
      "151 0.8572733918128655\n",
      "152 0.8331274703557312\n",
      "153 0.7748257839721254\n",
      "154 0.6748007123473542\n",
      "155 0.7496634615384615\n",
      "156 0.9397321428571429\n",
      "157 0.7995302287581699\n",
      "158 0.8916666666666667\n",
      "159 0.8249588815789474\n",
      "160 0.7428977272727273\n",
      "161 0.7735918972332015\n",
      "162 0.7744116149749374\n",
      "163 0.7810051638176638\n",
      "164 0.8011242540494459\n",
      "165 0.8391812865497076\n",
      "166 0.8107561383928571\n",
      "167 0.6936600221483943\n",
      "168 0.7789656432748538\n",
      "169 0.8123511904761904\n",
      "170 0.8189897486772487\n",
      "171 0.7789137260765551\n",
      "172 0.8925895467836257\n",
      "173 0.8137967562828755\n",
      "174 0.8972222222222223\n",
      "175 0.7755608974358974\n",
      "176 0.8208806818181819\n",
      "177 0.7759395424836601\n",
      "178 0.8334571139825676\n",
      "179 0.851197335584263\n",
      "180 0.7811266447368421\n",
      "181 0.791749824929972\n",
      "182 0.8173624375550984\n",
      "183 0.8639823717948718\n",
      "184 0.7686277636054422\n",
      "185 0.7345401691331924\n",
      "186 0.7859221032959565\n",
      "187 0.8409825058072009\n",
      "188 0.8930555555555556\n",
      "189 0.828125\n",
      "190 0.9322916666666666\n",
      "191 0.9672619047619048\n",
      "192 0.7799107142857142\n",
      "193 0.8108387945396437\n",
      "194 0.8268785612535613\n",
      "195 0.9235237418831169\n",
      "196 0.8149207239871087\n",
      "197 0.8228586212993221\n",
      "198 0.6986678685897436\n",
      "199 0.782717803030303\n",
      "200 0.871855389741728\n",
      "201 0.7900390625\n",
      "202 0.796895667989418\n",
      "203 0.8888888888888888\n",
      "204 0.7677399752160651\n",
      "205 0.7491892069327731\n",
      "206 0.7945899209486166\n",
      "207 0.773936449579832\n",
      "208 0.7360733695652174\n",
      "209 0.8839024511474665\n",
      "210 0.8669712070874862\n",
      "211 0.8489583333333334\n",
      "212 0.732081228956229\n",
      "213 0.7939703525641025\n",
      "214 0.8230077214452215\n",
      "215 0.8141084558823529\n",
      "216 0.804985119047619\n",
      "217 0.791890389876881\n",
      "218 0.7573563664596273\n",
      "219 0.7811880288207298\n",
      "220 0.834460999150934\n",
      "221 0.8778409090909091\n",
      "222 0.8276897176490302\n",
      "223 0.716448252688172\n",
      "224 0.8695913461538461\n",
      "225 0.8419117647058824\n",
      "226 0.8326998873873874\n",
      "227 0.7654297508925674\n",
      "228 0.8076822916666667\n",
      "229 0.8671875\n",
      "230 0.9271474753694581\n",
      "231 0.7992093656156156\n",
      "232 0.7394889684395091\n",
      "233 0.8074164603960396\n",
      "234 0.7945667613636364\n",
      "235 0.6798710724043716\n",
      "236 0.7972771139705882\n",
      "237 0.7683189655172413\n",
      "238 0.820850577252759\n",
      "239 0.8444901315789474\n",
      "240 0.7943048469387755\n",
      "241 0.8632831181252989\n",
      "242 0.7877375730994152\n",
      "243 0.8861111111111111\n",
      "244 0.7579458841463415\n",
      "245 0.7727578288596992\n",
      "246 0.8559802439532944\n",
      "247 0.9385416666666667\n",
      "248 0.8111563407111756\n",
      "249 0.6914663461538462\n",
      "250 0.8382498678646935\n",
      "251 0.7743509615384615\n",
      "252 0.9295691287878788\n",
      "253 0.8469351453726454\n",
      "254 0.8776400862068966\n",
      "255 0.7746212121212122\n",
      "256 0.7566636029411765\n",
      "257 0.7406332325588777\n",
      "258 0.8127533922939321\n",
      "259 0.714432123655914\n",
      "260 0.7920673076923077\n",
      "261 0.7769574175824175\n",
      "262 0.8221590909090909\n",
      "263 0.7286401797829036\n",
      "264 0.807421875\n",
      "265 0.7775297619047619\n",
      "266 0.8070427389705882\n",
      "267 0.7923368566176471\n",
      "268 0.8023686189829192\n",
      "269 0.7842092803030303\n",
      "270 0.8710813492063492\n",
      "271 1.0\n",
      "272 0.8884868421052632\n",
      "273 0.8174754382941003\n",
      "274 0.8719084224598931\n",
      "275 0.7803841645353794\n",
      "276 0.7830180921052632\n",
      "277 0.7892550158175158\n",
      "278 0.6660564426593522\n",
      "279 0.8660626323826413\n",
      "280 0.8380575501139542\n",
      "281 0.7934244791666667\n",
      "282 0.8597184065934066\n",
      "283 0.8273668248945147\n",
      "284 0.8365980957372466\n",
      "285 0.7475914031620553\n",
      "286 0.8369266333098219\n",
      "287 0.6613963293650794\n",
      "288 0.8320784684065934\n",
      "289 0.8485795454545455\n",
      "290 0.8714054987980769\n",
      "291 0.7782431722689076\n",
      "292 0.7663513321995464\n",
      "293 0.8145656426906427\n",
      "294 0.7668154761904762\n",
      "295 0.7619239267676767\n",
      "296 0.8022836538461539\n",
      "297 0.8584558823529411\n",
      "298 0.7606326804915514\n",
      "299 0.6433823529411765\n",
      "300 1.0\n",
      "301 0.7220165505226481\n",
      "302 0.8767478415915916\n",
      "303 0.844555942712906\n",
      "304 0.8675271739130435\n",
      "305 0.8218695287423402\n",
      "306 0.8880208333333334\n",
      "307 0.711668080693816\n",
      "308 0.8517754133357558\n",
      "309 0.808984375\n",
      "310 0.8759191176470589\n",
      "311 0.8428914835164835\n",
      "312 0.8008145513640239\n",
      "313 0.8408609053989489\n",
      "314 0.8064236111111112\n",
      "315 0.8954676150121066\n",
      "316 0.8722222222222222\n",
      "317 0.8146551724137931\n",
      "318 0.8245218323343323\n",
      "319 0.8221894392730497\n",
      "320 0.6782782146160962\n",
      "321 0.8394626524390244\n",
      "322 0.874631734006734\n",
      "323 0.7412860576923077\n",
      "324 0.6944691051136364\n",
      "325 0.8652129120879121\n",
      "326 0.8910256410256411\n",
      "327 0.7810391363022942\n",
      "328 0.8036275584795322\n",
      "329 0.7598549836601307\n",
      "330 0.8183001893939394\n",
      "331 1.0\n",
      "332 0.9054276315789473\n",
      "333 0.8545726940313207\n",
      "334 0.8272879464285714\n",
      "335 0.8943452380952381\n",
      "336 0.802734375\n",
      "337 0.9077719155844156\n",
      "338 0.8735795454545454\n",
      "339 0.8108477011494253\n",
      "340 0.7851865079365079\n",
      "341 0.8682892628205128\n",
      "342 0.9296875\n",
      "343 0.8129006410256411\n",
      "344 0.7103049089068826\n",
      "345 0.7990327380952381\n",
      "346 0.8961038961038961\n",
      "347 0.8857996323529411\n",
      "348 0.7365564123376623\n",
      "349 0.825020032051282\n",
      "350 0.7388321314102564\n",
      "351 0.8571802375256322\n",
      "352 0.8787147177419354\n",
      "353 0.8972222222222223\n",
      "354 0.7786997126436782\n",
      "355 0.8670691287878788\n",
      "356 0.8554370156855393\n",
      "357 0.691917251975417\n",
      "358 0.95703125\n",
      "359 0.729829863704257\n",
      "360 0.7780213046757164\n",
      "361 0.8871193910256411\n",
      "362 0.8915719696969697\n",
      "363 0.7654003769284713\n",
      "364 0.8727155264279625\n",
      "365 0.7863492918719212\n",
      "366 0.8530110035551434\n",
      "367 0.8204147296831956\n",
      "368 0.8334865196078431\n",
      "369 0.7651219818913481\n",
      "370 0.7430275537634409\n",
      "371 0.721361092641844\n",
      "372 0.8574774916247906\n",
      "373 0.8111430921052631\n",
      "374 0.8397959539892828\n",
      "375 0.8597450657894737\n",
      "376 0.7546977124183006\n",
      "377 0.8503472222222223\n",
      "378 0.8218947086696502\n",
      "379 0.7461138278126784\n",
      "380 0.8316865808823529\n",
      "381 0.7659243851049191\n",
      "382 0.8524960176161919\n",
      "383 0.8004807692307693\n",
      "384 0.7047091013824884\n",
      "385 0.7252013384754991\n",
      "386 0.8022569444444444\n",
      "387 0.8914030612244898\n",
      "388 0.7759453052728955\n",
      "389 0.7089937754535357\n",
      "390 0.8428843194254446\n",
      "391 0.8715773809523809\n",
      "392 0.7367294426048565\n",
      "393 0.7955785943556202\n",
      "394 0.9040464743589743\n",
      "395 0.8331006328529195\n",
      "396 0.8019831730769231\n",
      "397 0.8386819639606327\n",
      "398 0.9077635327635327\n",
      "399 0.8465647977941176\n",
      "400 0.7488871082621082\n",
      "401 0.8329529968156304\n",
      "402 0.8415568752163378\n",
      "403 0.7240473646723646\n",
      "404 0.7813426383399209\n",
      "405 0.7850911458333333\n",
      "406 0.8524305555555556\n",
      "407 0.8475855130784709\n",
      "408 0.9440040024630542\n",
      "409 0.7813682893913959\n",
      "410 0.810889423076923\n",
      "411 0.8974673202614379\n",
      "412 0.8186434095716552\n",
      "413 0.971875\n",
      "414 0.7997115384615384\n",
      "415 0.7770280550146057\n",
      "416 0.8828125\n",
      "417 0.7484315814393939\n",
      "418 0.8715277777777778\n",
      "419 0.8654532113131133\n",
      "420 0.715749430649427\n",
      "421 0.7099521396396397\n",
      "422 0.8855220985155196\n",
      "423 0.8985795454545454\n",
      "424 0.8514209692028986\n",
      "425 0.6664003314393939\n",
      "426 0.8555377565061493\n",
      "427 0.7746279761904762\n",
      "428 0.9160539215686274\n",
      "429 1.0\n",
      "430 0.8648454244711583\n",
      "431 0.718285990243555\n",
      "432 0.8524147727272727\n",
      "433 0.8818359375\n",
      "434 0.8241621376811594\n",
      "435 0.8816964285714286\n",
      "436 0.710021551724138\n",
      "437 0.8253472222222222\n",
      "438 0.7875\n",
      "439 0.7395114942528735\n",
      "440 0.8361272773279352\n",
      "441 0.6903123163385249\n",
      "442 0.8145148026315789\n",
      "443 0.8246017156862745\n",
      "444 0.8249797077922078\n",
      "445 0.8846713541982879\n",
      "446 0.8782857736697965\n",
      "447 0.7855415603741497\n",
      "448 0.7781723484848485\n",
      "449 0.8006214134231537\n",
      "450 0.8353125\n",
      "451 0.739546130952381\n",
      "452 0.802264847083926\n",
      "453 0.7588290998217468\n",
      "454 0.953215737514518\n",
      "455 0.7497785931174089\n",
      "456 0.796875\n",
      "457 0.88351898595259\n",
      "458 0.7643509615384615\n",
      "459 0.726927298553719\n",
      "460 0.8414259131368939\n",
      "461 0.9564732142857143\n",
      "462 0.7196151129943503\n",
      "463 0.8982938218390805\n",
      "464 0.7724538438256658\n",
      "465 0.8017218137254902\n",
      "466 0.9010416666666666\n",
      "467 0.7927924430641822\n",
      "468 0.9\n",
      "469 0.7590029761904762\n",
      "470 0.792299257759784\n",
      "471 0.8349454365079365\n",
      "472 0.7860555178652193\n",
      "473 0.7956801470588235\n",
      "474 0.7741477272727273\n",
      "475 0.8201791158536585\n",
      "476 0.752734375\n",
      "477 0.89609375\n",
      "478 0.8922988221399387\n",
      "479 0.8905066287878788\n",
      "480 0.8388083584337349\n",
      "481 0.9582950367647058\n",
      "482 0.8721328509852216\n",
      "483 0.8020833333333334\n",
      "484 0.7163822730179028\n",
      "485 0.8181818181818182\n",
      "486 0.8171977124183006\n",
      "487 0.7739044540229885\n",
      "488 0.7722128378378378\n",
      "489 0.7405875997340425\n",
      "490 0.8855962377922463\n",
      "491 0.975\n",
      "492 0.6785239361702128\n",
      "493 0.8279079861111112\n",
      "494 0.942193675889328\n",
      "495 0.7955439814814815\n",
      "496 0.848457532051282\n",
      "497 0.8297576031315869\n",
      "498 0.7596614583333333\n",
      "499 1.0\n",
      "Overall Average Jaccard Similarity: 0.8257428181078085\n"
     ]
    }
   ],
   "source": [
    "get_averege_jaccard_sim(final_buckets, minhashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexer = StringIndexer(inputCol=\"features\", outputCol=\"from_to_type_index\")\n",
    "# indexed_data = indexer.fit(actual_routes_feature).transform(actual_routes_feature)\n",
    "# assembler = VectorAssembler(inputCols=[\"from_to_type_index\"], outputCol=\"vector\")\n",
    "# actual_feature_data = assembler.transform(indexed_data)\n",
    "\n",
    "# actual_feature_data.show()\n",
    "# # def is_non_zero_vector(vector):\n",
    "# #     return vector.numNonzeros() > 0\n",
    "\n",
    "# # is_non_zero_vector_udf = udf(is_non_zero_vector, BooleanType())\n",
    "\n",
    "# # filtered_data = actual_feature_data.filter(is_non_zero_vector_udf(col(\"vector\")))\n",
    "\n",
    "\n",
    "# mh = MinHashLSH(inputCol=\"vector\", outputCol=\"hashes\", numHashTables=5, seed=1003)\n",
    "# model = mh.fit(actual_feature_data)\n",
    "\n",
    "# #transformed_filtered_data = model.transform(actual_feature_data).head()\n",
    "# test = model.approxNearestNeighbors()\n",
    "\n",
    "# # transformed_filtered_data.show(truncate=False, n=50)\n",
    "\n",
    "\n",
    "# #similar_items.show(truncate=False)\n",
    "\n",
    "# def is_non_zero_vector(vector):\n",
    "#     return vector.numNonzeros() > 0\n",
    "\n",
    "# from collections import defaultdict\n",
    "\n",
    "# representative_mapping = {}\n",
    "\n",
    "# group_mapping = defaultdict(list)\n",
    "\n",
    "# # Iterate over the user neighbors dictionary\n",
    "# for user, neighbors in new_process_dictionary.items():\n",
    "#     neighbors_sorted = tuple(sorted(neighbors))\n",
    "#     if neighbors_sorted in representative_mapping:\n",
    "#         representative = representative_mapping[neighbors_sorted]\n",
    "#     else:\n",
    "#         representative = neighbors_sorted[0]\n",
    "#         for neighbor in neighbors_sorted:\n",
    "#             representative_mapping[neighbor] = representative\n",
    "    \n",
    "#     representative_mapping[user] = representative\n",
    "#     group_mapping[representative].append(user)\n",
    "\n",
    "# new_user_neighbors = {}\n",
    "# for representative, users in group_mapping.items():\n",
    "#     new_user_neighbors[representative] = users\n",
    "\n",
    "# #print(new_user_neighbors)\n",
    "# new_users = []\n",
    "# for key, value in new_user_neighbors.items():\n",
    "#     new_users.append(key)\n",
    "\n",
    "# print(len(new_users))\n",
    "# filtered_df = df_grouped[df_grouped['user_id'].isin(new_users)]\n",
    "# print(f\"Amount of processes: {filtered_df.count()}\")\n",
    "#     shingles = shingle(features[\"features\"], 5)\n",
    "#     m = MinHash(num_perm=128)\n",
    "#     for shingle_item in shingles:\n",
    "#         m.update(shingle_item.encode(\"utf8\"))\n",
    "#     minhashes[int(features[\"user_id\"])] = m\n",
    "#     lsh.insert(int(features[\"user_id\"]), m)\n",
    "#     neigbours = lsh.query(m)\n",
    "#     print(features[\"user_id\"], neigbours)\n",
    "#     final_buckets[features[\"user_id\"]] = neigbours\n",
    "\n",
    "# print(final_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
