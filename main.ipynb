{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation import *\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,concat_ws, collect_list, lit,split, size, avg, udf, row_number, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "#from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "#from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy import average\n",
    "import numpy as np \n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "\n",
    "#os.environ[\"JAVA_HOME\"] = \"C:/Program Files/OpenJDK/jdk-21.0.2/bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def generate_case(tasks, case_id, start_time,min_datapoints,rand=True,connect = True):\n",
    "    '''\n",
    "    '''\n",
    "    case = []\n",
    "    time = start_time\n",
    "    case_id=str(case_id).zfill(len(str(min_datapoints)))\n",
    "    case.append(['null','S0',time,'Req',case_id]) #request between the user and the first server\n",
    "    \n",
    "    num_opts = len([key for key,values in tasks.items() if values[0] == 'opt'])\n",
    "    #random_indexes = sorted(random.sample(list(range(len(tasks.keys()))),random.randint(2, len(tasks.keys()))))\n",
    "    #list_new_tasks = [list(tasks.keys())[i] for i in sorted(random.sample(list(range(len(tasks.keys()))),random.randint(2, len(tasks.keys()))))]\n",
    "    if rand == True:\n",
    "        new_tasks = {task:tasks[task] for task in [list(tasks.keys())[i] for i in random.sample(list(range(len(tasks.keys())-num_opts)),random.randint(2, len(tasks.keys())-num_opts))]}\n",
    "    else:\n",
    "        new_tasks = tasks#{task:tasks[task] for task in [list(tasks.keys())[i] for i in sorted(random.sample(list(range(len(tasks.keys()))),random.randint(2, len(tasks.keys()))))]}\n",
    "    for task,subtasks in new_tasks.items():\n",
    "        if subtasks[0] != 'opt':\n",
    "            server = f\"S{list(tasks.keys()).index(task)+1}\" #SX\n",
    "            case.append(['S0',server,time,'Req',case_id])#request between the first server and the specific server\n",
    "        if subtasks[0]=='one':\n",
    "            service_task = random.choice(subtasks[1])\n",
    "            new_server = f\"{server}_{subtasks[1].index(service_task)+1}\"\n",
    "            #Request\n",
    "            case.append([server, new_server,time,'Req',case_id])\n",
    "            #Response\n",
    "            time += timedelta(microseconds=random.randint(subtasks[-1][0], subtasks[-1][1])*100000)\n",
    "            case.append([new_server,server,time,'Res',case_id]) #1 milisecond= 1000 microseconds\n",
    "            \n",
    "            \n",
    "        elif subtasks[0] == 'rand':\n",
    "            rand_tasks = random.sample(subtasks[1], random.randint(1, len(subtasks[1])))\n",
    "            for new_task in rand_tasks:\n",
    "                new_server = f\"{server}_{subtasks[1].index(new_task)+1}\"\n",
    "                #Request\n",
    "                case.append([server, new_server,time,'Req',case_id])\n",
    "                #Response\n",
    "                time += timedelta(microseconds=random.randint(subtasks[-1][0], subtasks[-1][1])*100000)\n",
    "                case.append([new_server,server,time,'Res',case_id])\n",
    "\n",
    "\n",
    "        elif subtasks[0] == 'all':\n",
    "            for new_task in subtasks[1]:\n",
    "                new_server = f\"{server}_{subtasks[1].index(new_task)+1}\"\n",
    "                case.append([server, new_server,time,'Req',case_id])\n",
    "                time += timedelta(microseconds=random.randint(subtasks[-1][0], subtasks[-1][1])*100000)\n",
    "                case.append([new_server,server,time,'Res',case_id]) \n",
    "        \n",
    "        elif subtasks[0] == 'con' and connect == True:\n",
    "            service_task = random.choice(subtasks[1])\n",
    "            new_server = f\"{server}_{subtasks[1].index(service_task)+1}\"\n",
    "            #Req\n",
    "            case.append([server, new_server,time,'Req',case_id])\n",
    "\n",
    "            #Opt\n",
    "            #Req\n",
    "            sub_service_task =random.choice(tasks[service_task][1])\n",
    "            sub_server = f\"{new_server}_{list(tasks[service_task][1]).index(sub_service_task)+1}\"\n",
    "            case.append([new_server,sub_server,time,'Req',case_id])\n",
    "            #Res\n",
    "            time += timedelta(microseconds=random.randint(tasks[service_task][-1][0], tasks[service_task][-1][1])*100000)\n",
    "            case.append([sub_server,new_server,time,'Res',case_id])\n",
    "            #Opt end\n",
    "\n",
    "            #Res\n",
    "            case.append([new_server,server,time,'Res',case_id])\n",
    "        \n",
    "        elif subtasks[0] == 'opt':\n",
    "            pass\n",
    "        #else:\n",
    "            #print('smth wrong')#need to change this to make sure it raises an error or smth like that\n",
    "        \n",
    "        if subtasks[0] != 'opt':\n",
    "            case.append([server,'S0',time,'Res',case_id])#response between the specific server and the first server \n",
    "    case.append(['S0','null',time,'Res',case_id])#response between and the first server the user\n",
    "    return case\n",
    "\n",
    "\n",
    "#Random times\n",
    "def random_time(start_time,end_time):\n",
    "    delta = end_time - start_time\n",
    "    return start_time + timedelta(seconds=random.randint(0, int(delta.total_seconds())))\n",
    "\n",
    "def generate_dataset(tasks, min_datapoints,start_time,end_time,random=True,connect = True,file_name = './SGD file.csv'):\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    datapoints = 0\n",
    "    dataset = []\n",
    "    user_id = 1\n",
    "    while datapoints <= min_datapoints:\n",
    "        time = random_time(start_time,end_time)\n",
    "        case = generate_case(tasks, user_id, time,min_datapoints, random, connect= connect)\n",
    "        dataset.extend(case)\n",
    "        datapoints+= len(case)\n",
    "        user_id+=1\n",
    "    \n",
    "    df = pd.DataFrame(dataset, columns=['from', 'to', 'timestamp', 'type', 'user_id'])\n",
    "    output_file = \"data/SDG_\"+file_name+\".csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "tasks = {'log_in': ['one',['credentials check','sing up','recover pw and log in'],(0,10)],\n",
    "            'search_book': ['rand',['history','fantasy','crime','poetry','biography'],(5,15)],\n",
    "            'shipment' : ['all',['adress','door number','zip code'],(15,50)],\n",
    "            'payment' : ['one',['visa','master card','revolut','paypal','apple pay'],(10,100)],\n",
    "            'new_site' : ['con',['site1','site2'],(10,100)],\n",
    "            'site1' : ['opt',['site1_1','site1_2','site1_3'],(10,100)],\n",
    "            'site2' : ['opt',['site2_1','site2_2'],(10,100)]\n",
    "}   \n",
    "start_time = datetime(2024, 6, 3, 9, 0, 0)  \n",
    "end_time = datetime(2024, 6, 3, 10, 45, 0) \n",
    "\n",
    "# #Experiment 1\n",
    "generate_dataset(tasks, 1000,start_time,end_time,file_name=\"dataset1\")  \n",
    "# #Experiment 2: con\n",
    "# generate_dataset(tasks, 1000000,start_time,end_time,random=False,file_name=\"dataset1\")  \n",
    "# #Experiment 3\n",
    "#generate_dataset(tasks, 1000000,start_time,end_time,file_name=\"dataset2\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "################################################# Question 1 ###################################################################\n",
    "################################################################################################################################\n",
    "def get_memory_usage(): \n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n",
    "\n",
    "def get_cpu_usage(): \n",
    "    return psutil.cpu_percent(interval=None)\n",
    "\n",
    "def get_performance(func1,func2, vals):\n",
    "    #k_values = [2, 3, 4, 5, 6, 7, 8]\n",
    "    results = []\n",
    "\n",
    "    for k in vals:\n",
    "        start_time = time.time()\n",
    "        start_mem = get_memory_usage()\n",
    "        start_cpu = get_cpu_usage()\n",
    "\n",
    "        replacement_candidates, minhashes = func1(df_grouped, k, 0.98)\n",
    "        new_process_dictionary = func2(replacement_candidates)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        end_mem = get_memory_usage()\n",
    "        end_cpu = get_cpu_usage()\n",
    "\n",
    "        duration = end_time - start_time\n",
    "        mem_used = end_mem - start_mem\n",
    "\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'time_seconds': duration,\n",
    "            'memory_mb': mem_used,\n",
    "            'unique_processes': len(new_process_dictionary),\n",
    "            'cpu': end_cpu\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def plot_results(results):\n",
    "    k_values = [result['k'] for result in results]\n",
    "    time_seconds = [result['time_seconds'] for result in results]\n",
    "    cpu_percentages = [result['cpu'] for result in results]\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax1.set_xlabel('k values')\n",
    "    ax1.set_ylabel('Time (seconds)', color='tab:blue')\n",
    "    ax1.plot(k_values, time_seconds, marker='o', color='tab:blue', label='Time')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('CPU Usage (%)', color='tab:red')\n",
    "\n",
    "    ax2.plot(k_values, cpu_percentages, marker='^', color='tab:red', linestyle='--', label='CPU Usage')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    plt.title('Performance Metrics for Different k Values')\n",
    "    fig.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_performances(results):\n",
    "    k_values = [result['k'] for result in results]\n",
    "    time_seconds = [result['time_seconds'] for result in results]\n",
    "    cpu_percentages = [result['cpu'] for result in results]\n",
    "\n",
    "    # Calculate the performance metric (Product of time_seconds and cpu)\n",
    "    performance_metric = [time_seconds[i] * cpu_percentages[i] for i in range(len(results))]\n",
    "\n",
    "    # Create a figure and axis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the performance metric\n",
    "    plt.plot(k_values, performance_metric, marker='o', linestyle='-', color='purple', label='Time * CPU')\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('k values')\n",
    "    plt.ylabel('Performance Metric (Time * CPU)')\n",
    "    plt.title('Combined Metric of Time and CPU Usage vs. k Values')\n",
    "    plt.xticks(k_values)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def shingle(text, k=7):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text)-k +1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return list(set(shingle_set))\n",
    "\n",
    "def jaccard_similarity(list1, list2):   \n",
    "    return len(set(list1).intersection(set(list2))) / len(set(list1).union(set(list2)))\n",
    "\n",
    "def minhash_lsh(df, k_shingle, threshold):\n",
    "\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
    "    minhashes = {}\n",
    "\n",
    "    for features in df.collect():\n",
    "        shingles = shingle(features[\"features\"], k_shingle)\n",
    "        m = MinHash(num_perm=128)\n",
    "        for shingle_item in shingles:\n",
    "            m.update(shingle_item.encode(\"utf8\"))\n",
    "        minhashes[int(features[\"user_id\"])] = m\n",
    "        lsh.insert(int(features[\"user_id\"]), m)\n",
    "\n",
    "    replacement_candidates = {}\n",
    "    for key in lsh.keys: \n",
    "        replacement_candidates[key] = lsh.query(minhashes[key]) \n",
    "\n",
    "    #Key: New representative, value: Similar items\n",
    "    return replacement_candidates,minhashes\n",
    "\n",
    "\n",
    "def remove_dups(graph):\n",
    "    keys_to_remove = []\n",
    "    for key, values in graph.items():\n",
    "        # Remove the key from its own value list if present\n",
    "        if key in values:\n",
    "            values.remove(key)\n",
    "        # Mark keys for removal if their value list is empty\n",
    "        if not values:\n",
    "            keys_to_remove.append(key)\n",
    "        else:\n",
    "            graph[key] = values  # Update the graph with the cleaned value list\n",
    "    \n",
    "    # Remove keys that had empty value lists\n",
    "    for key in keys_to_remove:\n",
    "        graph.pop(key)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def bucketing(replacement_candidates):\n",
    "    #replacement_candidates = remove_dups(replacement_candidates.copy())\n",
    "    visited_processes = set()\n",
    "    new_process_dictionary = {}\n",
    "\n",
    "    def bfs(start):\n",
    "        queue = [start]\n",
    "        bucket = []\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            if current not in visited_processes:\n",
    "                visited_processes.add(current)\n",
    "                bucket.append(current)\n",
    "                if current in replacement_candidates:\n",
    "                    for neighbor in replacement_candidates[current]:\n",
    "                        if neighbor not in visited_processes:\n",
    "                            queue.append(neighbor)\n",
    "        return bucket\n",
    "\n",
    "    for key in replacement_candidates.keys():\n",
    "        if key not in visited_processes:\n",
    "            bucket = bfs(key)\n",
    "            if bucket:\n",
    "                new_process_dictionary[key] = sorted(bucket)\n",
    "\n",
    "    return new_process_dictionary\n",
    "\n",
    "\n",
    "\n",
    "def get_case(caseID,data):\n",
    "    data1 = data.filter(data.user_id.isin([caseID]))\n",
    "    data1.show()\n",
    "\n",
    "def compare_cases(case1,case2,data):\n",
    "    data1 = data.filter(data.user_id.isin([case1]))\n",
    "    data2 = data.filter(data.user_id.isin([case2]))\n",
    "    desired_column_list1 = data1.select(\"to\").rdd.flatMap(lambda x: x).collect()\n",
    "    desired_column_list2 = data2.select(\"to\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    common_elements = np.intersect1d(desired_column_list1, desired_column_list2)\n",
    "    union_elements = np.union1d(desired_column_list1, desired_column_list2)\n",
    "    print(len(common_elements)/len(union_elements))\n",
    "\n",
    "def get_traces(user_id,df):\n",
    "    result = df.filter(col(\"user_id\") == user_id).select(\"features\").collect()\n",
    "    if result:\n",
    "        return result[0][\"features\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_shingles(user_id,df):\n",
    "    result = df.filter(col(\"user_id\") == user_id).select(\"shingles\").collect()\n",
    "    if result:\n",
    "        return result[0][\"shingles\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def write_df(df, file_name):\n",
    "    # Ensure the Output directory exists\n",
    "    os.makedirs('Output', exist_ok=True)\n",
    "    \n",
    "\n",
    "    columns = df.columns\n",
    "    columns.remove('user_id')\n",
    "    new_column_order = columns + ['user_id']\n",
    "    df = df.select(new_column_order)\n",
    "    \n",
    "    # Coalesce to a single partition and write the DataFrame to a text file\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv('temp/temp_output')\n",
    "    \n",
    "    # Find the part file and move it to the desired location\n",
    "    part_file = [f for f in os.listdir('temp/temp_output') if f.startswith(\"part-\")][0]\n",
    "    shutil.move(os.path.join('temp/temp_output', part_file), file_name)\n",
    "    shutil.rmtree('temp/temp_output')\n",
    "\n",
    "def output_part1(dataset, k, threshold,q=1):\n",
    "    spark = SparkSession.builder.appName(\"OutputUserIDs\").getOrCreate()\n",
    "    data = spark.read.csv(dataset, header=True, inferSchema=True)\n",
    "    df_filtered_m = data.filter(data.type.isin(['Req']))\n",
    "    df_grouped = df_filtered_m.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"features\"))\n",
    "    \n",
    "    replacement_candidates = minhash_lsh(df_grouped, k, threshold)\n",
    "    new_process_dictionary = bucketing(replacement_candidates[0])    \n",
    "\n",
    "    \n",
    "    user_ids_to_change = []#[key for key in new_process_dictionary.keys()]#[key for key, values in new_process_dictionary.items() if len(values) > 1 or ]\n",
    "    for key, values in new_process_dictionary.items():\n",
    "        if len(values) >1:\n",
    "            user_ids_to_change.append(key)\n",
    "        elif len(values) == 1 and values[0]!= key:\n",
    "            user_ids_to_change.append(key)\n",
    "\n",
    "    user_ids_to_delete = []\n",
    "    for id in user_ids_to_change:\n",
    "        for case in new_process_dictionary[id]:\n",
    "            if case not in user_ids_to_change:\n",
    "                user_ids_to_delete.append(case)\n",
    "\n",
    "    max_user_id = df_grouped.select(spark_max(\"user_id\")).collect()[0][0]\n",
    "\n",
    "    final_l = user_ids_to_change+user_ids_to_delete\n",
    "\n",
    "    df_to_change = df_grouped.filter(col(\"user_id\").isin(user_ids_to_change))\n",
    "    distinct_user_ids_to_change = df_to_change.select(\"user_id\").distinct()\n",
    "    window_spec = Window.orderBy(\"user_id\")\n",
    "    user_id_mapping = distinct_user_ids_to_change.withColumn(\"new_user_id\", row_number().over(window_spec) + max_user_id )\n",
    "    \n",
    "    df_with_new_ids = data.join(user_id_mapping, on=\"user_id\", how=\"left\")\n",
    "    df_with_new_ids = df_with_new_ids.withColumn(\"user_id\",\n",
    "                                                 when(col(\"new_user_id\").isNotNull(), col(\"new_user_id\"))\n",
    "                                                 .otherwise(col(\"user_id\"))) \\\n",
    "                                     .drop(\"new_user_id\")\n",
    "\n",
    "    columns = df_with_new_ids.columns\n",
    "    columns.remove('user_id')\n",
    "    new_column_order = columns + ['user_id']\n",
    "    df_with_new_ids = df_with_new_ids.select(new_column_order)\n",
    "    \n",
    "    return df_with_new_ids, user_ids_to_delete, user_ids_to_change\n",
    "\n",
    "\n",
    "################################################################################################################################\n",
    "################################################# Question 2 ###################################################################\n",
    "################################################################################################################################\n",
    "\n",
    "def kmeans_clustering(df, n_clusters, max_iter):\n",
    "    minhashes = []\n",
    "    #for jaccard verification\n",
    "    minhash_dict = {}\n",
    "    user_ids = []\n",
    "    final_buckets = {}\n",
    "    for features in df.collect():\n",
    "        shingles = shingle(features[\"features\"], 5)\n",
    "        m = MinHash(num_perm=128)\n",
    "        for shingle_item in shingles:\n",
    "            m.update(shingle_item.encode(\"utf8\"))\n",
    "        minhashes.append(m.hashvalues)\n",
    "        minhash_dict[int(features[\"user_id\"])] = m\n",
    "        user_ids.append(int(features[\"user_id\"]))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, max_iter=max_iter).fit(minhashes)\n",
    "\n",
    "    user_clusters = dict(zip(user_ids, kmeans.labels_))\n",
    "    final_buckets = {}\n",
    "    for key, value in user_clusters.items():\n",
    "        if value in final_buckets:\n",
    "            final_buckets[value].append(key)\n",
    "        else:\n",
    "            final_buckets[value] = [key]\n",
    "\n",
    "    return final_buckets, minhash_dict\n",
    "\n",
    "def get_averege_jaccard_sim(final_buckets, minhashes,get = True):\n",
    "    sims = {}\n",
    "    for key, value in final_buckets.items():\n",
    "        for user_id_1 in final_buckets[key]:\n",
    "            for user_id_2 in final_buckets[key]:\n",
    "                if user_id_1 != user_id_2:\n",
    "                    sig_1 = minhashes[int(user_id_1)]\n",
    "                    sig_2 = minhashes[int(user_id_2)]\n",
    "                    sim = MinHash.jaccard(sig_1, sig_2)\n",
    "                    if key not in sims:\n",
    "                        sims[key] = [sim]\n",
    "                    else:\n",
    "                        sims[key].append(sim)\n",
    "    total_sum = 0\n",
    "    total_count = 0\n",
    "    sims = dict(sorted(sims.items()))\n",
    "    if get == True:\n",
    "        for key, value in sims.items():\n",
    "            avg_sim = average(value)\n",
    "            print(key, avg_sim)\n",
    "            total_sum += sum(value)\n",
    "            total_count += len(value)\n",
    "        \n",
    "        overall_average = total_sum / total_count if total_count != 0 else 0\n",
    "        print(\"Overall Average Jaccard Similarity:\", overall_average)\n",
    "\n",
    "    return sims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset(tasks, 10000,start_time,end_time,file_name=\"dataset1\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3,5,7,9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(text, k=7):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text)-k +1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return list(set(shingle_set))\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\"data/SDG_dataset2.csv\", header=True, inferSchema=True)\n",
    "df_filtered_m = data.filter(data.type.isin(['Req']))\n",
    "df_grouped = df_filtered_m.groupBy(\"user_id\").agg(concat_ws(\"\",collect_list(\"to\")).alias(\"features\"))\n",
    "\n",
    "shingles_udf = udf(shingle, ArrayType(StringType()))\n",
    "df_shingles = df_filtered_m.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"trace\")) \\\n",
    "    .withColumn(\"shingles\", shingles_udf(col(\"trace\"))) \\\n",
    "    .select(\"user_id\", \"shingles\")\n",
    "\n",
    "#data.show()\n",
    "#df_filtered_m.show()\n",
    "#df_grouped.show()\n",
    "#df_shingles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg number of shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_num_shingles = df_shingles.withColumn(\"list_length\", size(col(\"shingles\"))) \\\n",
    "                     .agg(avg(\"list_length\").alias(\"average_list_length\")).collect()[0][0]\n",
    "average_num_shingles\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do (avg-1)/avg and use thar as threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial number of cases: {df_grouped.count()}\")\n",
    "ans = minhash_lsh(df_grouped,7,0.97)\n",
    "replacement_candidates7, minhash_dic7 = ans[0],ans[1]\n",
    "new_process_dictionary7= bucketing(replacement_candidates7)\n",
    "print(f\"Number of unique processes after merging them with 0.97 threshold using 7-shingles: {len(new_process_dictionary7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the worst case scneario, with the smallest approximate jaccard sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = get_averege_jaccard_sim(replacement_candidates7, minhash_dic7,get=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dissimilar gives you the key for the buckets with the smallest jaccard sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = min(set(value for key,values in sims.items() for value in values if value != 1.0))\n",
    "final_values = []\n",
    "for key,values in sims.items():\n",
    "    for value in values:\n",
    "        if value == ans:\n",
    "            final_values.append(key)\n",
    "\n",
    "dissimilar = set(final_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the minimum Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THese are the cases of which the buckets have the sim above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dissimilar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANd here we get all the user_ids that have the smaller similarities and their key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sims = []\n",
    "for key in dissimilar:\n",
    "    for value in replacement_candidates7[key]:\n",
    "        new_sims.append((key,value,jaccard_similarity(get_shingles(value,df_shingles),get_shingles(key,df_shingles))))\n",
    "investigate = [case for case in new_sims if case[-1]!=1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in investigate:\n",
    "    print(f'######################### {case[0]} vs {case[1]} ################################')\n",
    "    print(get_traces(case[0],df_grouped))\n",
    "    print(get_traces(case[1],df_grouped))\n",
    "    print('#######################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of what I saw with Hugo. Repeat all this for each k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramter-tuning for k-shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to see which k is the best for the k-shingles. We gonna do such a thing by investigating how computational expensive it is to compute such minhash for different values of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_performance(minhash_lsh,bucketing, [3,4,5,6,7,8,9])\n",
    "plot_results(results)\n",
    "plot_performances(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running this a considerable amount of times we saw that the value that get a better balance between time and CPU usage is when k=7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter-tuning for the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the average number of 7-shingles is 32, and we want to group processes with only small variations, we want that 31/32 shingles to be the same, so that we still allow for some small variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial number of cases: {df_grouped.count()}\")\n",
    "ans = minhash_lsh(df_grouped,7,0.97)\n",
    "replacement_candidates7, minhash_dic = ans[0],ans[1]\n",
    "new_process_dictionary7= bucketing(replacement_candidates7)\n",
    "print(f\"Number of unique processes after merging them with 0.97 threshold using 7-shingles: {len(new_process_dictionary7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the processes with a threshold of 97%, using 7-shingles, we obtain 29729 candidate unique cases. In order to investigate if further analysis into the similarities needs to be done, to make sure that the false positives do not result in cases where the cases are not small variations of each other, we are going to compute the average similarities of all buckets and investigate the mininum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = get_averege_jaccard_sim(replacement_candidates7, minhash_dic,get=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to mention that we're still just computing the approximate jaccard similarities provided by MinHashLSH, so, in order to investigate the cases that have the smallest approximate jaccard similarities, we're going to compute the actual similarities between those cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = min(set(value for key,values in sims.items() for value in values if value != 1.0))\n",
    "final_values = []\n",
    "for key,values in sims.items():\n",
    "    for value in values:\n",
    "        if value == ans:\n",
    "            final_values.append(key)\n",
    "\n",
    "dissimilar = set(final_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in investigate:\n",
    "    print(f'######################### {case[0]} vs {case[1]} ################################')\n",
    "    print(get_traces(case[0],df_grouped))\n",
    "    print(get_traces(case[1],df_grouped))\n",
    "    print('#######################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[from: string, to: string, timestamp: timestamp, type: string, user_id: int],\n",
       " [203,\n",
       "  304,\n",
       "  96,\n",
       "  274,\n",
       "  356,\n",
       "  272,\n",
       "  275,\n",
       "  354,\n",
       "  421,\n",
       "  315,\n",
       "  395,\n",
       "  322,\n",
       "  345,\n",
       "  402,\n",
       "  367,\n",
       "  316,\n",
       "  223,\n",
       "  410,\n",
       "  353,\n",
       "  403,\n",
       "  420,\n",
       "  371,\n",
       "  400],\n",
       " [14,\n",
       "  23,\n",
       "  32,\n",
       "  68,\n",
       "  114,\n",
       "  117,\n",
       "  132,\n",
       "  146,\n",
       "  147,\n",
       "  154,\n",
       "  181,\n",
       "  185,\n",
       "  201,\n",
       "  240,\n",
       "  246,\n",
       "  276,\n",
       "  294,\n",
       "  346,\n",
       "  386])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_part1(\"data/SDG_dataset2.csv\",3,0.95)\n",
    "# output_part1(\"data/SDG_dataset2.csv\",5,0.95)\n",
    "output_part1(\"data/SDG_dataset1.csv\",7,0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{3: ['null', 'S0', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 13), 3, 'S0', 'S2', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 13), 3, 'S2', 'S2_5', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 13), 3, 'S2_5', 'S2', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 13, 600000), 3, 'S2', 'S0', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 13, 600000), 3, 'S0', 'S3', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 13, 600000), 3, 'S3', 'S3_1', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 13, 600000), 3, 'S3_1', 'S3', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 15, 600000), 3, 'S3', 'S3_2', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 15, 600000), 3, 'S3_2', 'S3', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 17, 300000), 3, 'S3', 'S3_3', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 17, 300000), 3, 'S3_3', 'S3', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 19, 100000), 3, 'S3', 'S0', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 19, 100000), 3, 'S0', 'S1', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 19, 100000), 3, 'S1', 'S1_2', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 19, 100000), 3, 'S1_2', 'S1', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 19, 900000), 3, 'S1', 'S0', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 19, 900000), 3, 'S0', 'S5', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 19, 900000), 3, 'S5', 'S5_2', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 19, 900000), 3, 'S5_2', 'S5_2_2', 'Req', datetime.datetime(2024, 6, 3, 9, 10, 19, 900000), 3, 'S5_2_2', 'S5_2', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 28, 300000), 3, 'S5_2', 'S5', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 28, 300000), 3, 'S5', 'S0', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 28, 300000), 3, 'S0', 'null', 'Res', datetime.datetime(2024, 6, 3, 9, 10, 28, 300000), 3]}, {45: ['null', 'S0', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 7), 45, 'S0', 'S3', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 7), 45, 'S3', 'S3_1', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 7), 45, 'S3_1', 'S3', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 8, 500000), 45, 'S3', 'S3_2', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 8, 500000), 45, 'S3_2', 'S3', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 12, 100000), 45, 'S3', 'S3_3', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 12, 100000), 45, 'S3_3', 'S3', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 13, 600000), 45, 'S3', 'S0', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 13, 600000), 45, 'S0', 'S1', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 13, 600000), 45, 'S1', 'S1_2', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 13, 600000), 45, 'S1_2', 'S1', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 14, 400000), 45, 'S1', 'S0', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 14, 400000), 45, 'S0', 'S5', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 14, 400000), 45, 'S5', 'S5_2', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 14, 400000), 45, 'S5_2', 'S5_2_2', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 14, 400000), 45, 'S5_2_2', 'S5_2', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 19, 400000), 45, 'S5_2', 'S5', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 19, 400000), 45, 'S5', 'S0', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 19, 400000), 45, 'S0', 'S4', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 19, 400000), 45, 'S4', 'S4_3', 'Req', datetime.datetime(2024, 6, 3, 9, 34, 19, 400000), 45, 'S4_3', 'S4', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 24), 45, 'S4', 'S0', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 24), 45, 'S0', 'null', 'Res', datetime.datetime(2024, 6, 3, 9, 34, 24), 45]}, {422: ['null', 'S0', 'Req', datetime.datetime(2024, 6, 3, 10, 26, 5), 422, 'S0', 'S3', 'Req', datetime.datetime(2024, 6, 3, 10, 26, 5), 422, 'S3', 'S3_1', 'Req', datetime.datetime(2024, 6, 3, 10, 26, 5), 422, 'S3_1', 'S3', 'Res', datetime.datetime(2024, 6, 3, 10, 26, 6, 800000), 422, 'S3', 'S3_2', 'Req', datetime.datetime(2024, 6, 3, 10, 26, 6, 800000), 422, 'S3_2', 'S3', 'Res', datetime.datetime(2024, 6, 3, 10, 26, 11, 600000), 422, 'S3', 'S3_3', 'Req', datetime.datetime(2024, 6, 3, 10, 26, 11, 600000), 422, 'S3_3', 'S3', 'Res', datetime.datetime(2024, 6, 3, 10, 26, 15, 100000), 422, 'S3', 'S0', 'Res', datetime.datetime(2024, 6, 3, 10, 26, 15, 100000), 422, 'S0', 'S1', 'Req', datetime.datetime(2024, 6, 3, 10, 26, 15, 100000), 422, 'S1', 'S1_2', 'Req', datetime.datetime(2024, 6, 3, 10, 26, 15, 100000), 422, 'S1_2', 'S1', 'Res', datetime.datetime(2024, 6, 3, 10, 26, 15, 700000), 422, 'S1', 'S0', 'Res', datetime.datetime(2024, 6, 3, 10, 26, 15, 700000), 422, 'S0', 'S5', 'Req', datetime.datetime(2024, 6, 3, 10, 26, 15, 700000), 422, 'S5', 'S5_2', 'Req', datetime.datetime(2024, 6, 3, 10, 26, 15, 700000), 422, 'S5_2', 'S5_2_2', 'Req', datetime.datetime(2024, 6, 3, 10, 26, 15, 700000), 422, 'S5_2_2', 'S5_2', 'Res', datetime.datetime(2024, 6, 3, 10, 26, 20, 900000), 422, 'S5_2', 'S5', 'Res', datetime.datetime(2024, 6, 3, 10, 26, 20, 900000), 422, 'S5', 'S0', 'Res', datetime.datetime(2024, 6, 3, 10, 26, 20, 900000), 422, 'S0', 'null', 'Res', datetime.datetime(2024, 6, 3, 10, 26, 20, 900000), 422]}]\n"
     ]
    }
   ],
   "source": [
    "def get_full_trace(user, df):\n",
    "    trace = []\n",
    "    filtered_df = df[df['user_id'].isin(user)]\n",
    "    for features in filtered_df.collect():\n",
    "        trace.append(features[\"from\"])\n",
    "        trace.append(features[\"to\"])\n",
    "        trace.append(features[\"type\"])\n",
    "        trace.append(features[\"timestamp\"])\n",
    "        trace.append(features[\"user_id\"])\n",
    "    return trace\n",
    "\n",
    "def output_part2(dataset, k, threshold,q=1):\n",
    "    spark = SparkSession.builder.appName(\"Output2\").getOrCreate()\n",
    "    data = spark.read.csv(dataset, header=True, inferSchema=True)\n",
    "    df_filtered_m = data.filter(data.type.isin(['Req']))\n",
    "    df_grouped = df_filtered_m.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"features\"))\n",
    "    \n",
    "    replacement_candidates, minhashes = minhash_lsh(df_grouped, k, threshold)\n",
    "    new_process_dictionary = bucketing(replacement_candidates)    \n",
    "\n",
    "    final_buckets = {}\n",
    "    index = 1\n",
    "    for _, value in new_process_dictionary.items():\n",
    "        if len(value) > 1:\n",
    "            final_buckets[index] = []\n",
    "            for v in value:\n",
    "                dict1 = {}\n",
    "                dict1[v] = []\n",
    "                dict1[v] += get_full_trace(v, data)\n",
    "                final_buckets[index].append(dict1)\n",
    "            index+=1\n",
    "    \n",
    "    #print(final_buckets)\n",
    "    return new_process_dictionary,final_buckets, minhashes\n",
    "\n",
    "\n",
    "new_process_dictionary,final_buckets, minhashes = output_part2(\"data/SDG_dataset1.csv\",7,0.7)\n",
    "\n",
    "\n",
    "print(final_buckets[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {}\n",
    "for key, values in final_buckets.items():\n",
    "    for value in values:\n",
    "        print(key, value)\n",
    " \n",
    "\n",
    "sims = get_averege_jaccard_sim(new_process_dictionary, minhashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Find and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_candidates = minhash_lsh(df_grouped,5,0.7)\n",
    "new_process_dictionary = bucketing(replacement_candidates)\n",
    "print(len(replacement_candidates))\n",
    "print(len(new_process_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Find/cluster similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "users = []\n",
    "for key in new_process_dictionary:\n",
    "    users.append(key)\n",
    "\n",
    "filtered_df = df_grouped[df_grouped['user_id'].isin(users)]\n",
    "\n",
    "final_buckets, minhashes = kmeans_clustering(filtered_df,500,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_averege_jaccard_sim(final_buckets, minhashes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Initial number of cases: {df_grouped.count()}\")\n",
    "# replacement_candidates3 = minhash_lsh(df_grouped,3,0.98)\n",
    "# new_process_dictionary3 = bucketing(replacement_candidates3)\n",
    "#print(f\"After merging cases with threshold 3-shingles: {len(new_process_dictionary3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split = df_grouped.withColumn(\"feature_length\", size(split(col(\"features\"), \"S\")))\n",
    "# average_length = df_split.agg(avg(\"feature_length\")).collect()[0][0]\n",
    "\n",
    "# windowSpec = Window.partitionBy(F.lit(1)).orderBy(\"feature_length\")\n",
    "# df_split = df_split.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "# total_count = df_split.count()\n",
    "\n",
    "# if total_count % 2 == 0:\n",
    "#     median_index1 = total_count // 2\n",
    "#     median_index2 = median_index1 + 1\n",
    "#     median_value = df_split.filter(col(\"row_number\").isin([median_index1, median_index2])) \\\n",
    "#                                   .agg(avg(\"feature_length\")).collect()[0][0]\n",
    "# else:\n",
    "#     median_index = (total_count // 2) + 1\n",
    "#     median_value = df_split.filter(col(\"row_number\") == median_index) \\\n",
    "#                                   .select(\"feature_length\").collect()[0][0]\n",
    "    \n",
    "# print(f\"Average number of requests: {average_length}\")\n",
    "# print(f\"Median number of requests: {median_value}\")\n",
    "\n",
    "\n",
    "#Given that both the average and the median are close to 13, which shows that the dataset is symetricly distributed when it comes to how many \n",
    "#requests are performed, we gonna assume that two cases have a small variation iif the number of different requests is around 1. To get an \n",
    "# #approximation of the threshold, we're going to use the resutls shown before, so we assume that 12/13 are the same. Given this, we decided \n",
    "# to use a threshold of 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_node_from_neighbors(graph, node_to_remove):\n",
    "    if node_to_remove in graph:\n",
    "        # Remove node_to_remove from all adjacency lists in the graph\n",
    "        for node, neighbors in graph.items():\n",
    "            if node != node_to_remove:  # Skip the node_to_remove itself\n",
    "                if node_to_remove in neighbors:\n",
    "                    neighbors.remove(node_to_remove)\n",
    "\n",
    "def dfs(graph, start, visited):\n",
    "    stack = [start]\n",
    "    result = []\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            result.append(node)\n",
    "            stack.extend(graph[node])\n",
    "    return result\n",
    "\n",
    "def transform_graph(graph):\n",
    "    reachable_dict = {}\n",
    "    for node in graph:\n",
    "        visited = set()\n",
    "        reachable_nodes = dfs(graph, node, visited)\n",
    "        reachable_dict[node] = [n for n in reachable_nodes if n != node]  # Exclude the start node itself\n",
    "    \n",
    "    # Filter out keys that also appear as values\n",
    "    keys_to_remove = set()\n",
    "    for node, neighbors in graph.items():\n",
    "        keys_to_remove.update(neighbors)\n",
    "    \n",
    "    final_dict = {k: v for k, v in reachable_dict.items() if k not in keys_to_remove}\n",
    "    return final_dict\n",
    "\n",
    "def bucketing(old_replacement_candidates):\n",
    "    alt_old_replacement_candidates = old_replacement_candidates.copy()\n",
    "    replacement_candidates = alt_old_replacement_candidates\n",
    "    print('no dups')\n",
    "    print(replacement_candidates)\n",
    "    visited = set()\n",
    "    for node in replacement_candidates.keys():\n",
    "        if node not in visited:\n",
    "            remove_node_from_neighbors(replacement_candidates, node)\n",
    "        for neighbour in replacement_candidates[node]:\n",
    "            visited.add(neighbour)\n",
    "    # print('no neighbours')\n",
    "    # print(replacement_candidates)\n",
    "    new_process_dictionary = transform_graph(replacement_candidates)\n",
    "    print('final')\n",
    "    print(new_process_dictionary)\n",
    "    \n",
    "        \n",
    "    return new_process_dictionary\n",
    "\n",
    "#generate_dataset(tasks, 100000,start_time,end_time,file_name=\"dataset1\",random=False,connect=False)  \n",
    "#print('minhash')\n",
    "ans = output_part1(\"data/SDG_dataset1.csv\", 5, 0.97)\n",
    "#print(ans[39])\n",
    "print(ans)\n",
    "#ans2 = bucketing(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
