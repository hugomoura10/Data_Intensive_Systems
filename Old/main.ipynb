{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation import *\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,concat_ws, collect_list, lit,split, size, avg, udf, row_number, when, date_format, length\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "#from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "#from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy import average\n",
    "import numpy as np \n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "\n",
    "#os.environ[\"JAVA_HOME\"] = \"C:/Program Files/OpenJDK/jdk-21.0.2/bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "################################################# Question 1 ###################################################################\n",
    "################################################################################################################################\n",
    "def get_memory_usage(): \n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n",
    "\n",
    "def get_cpu_usage(): \n",
    "    return psutil.cpu_percent(interval=None)\n",
    "\n",
    "def get_performance(func1,func2, vals):\n",
    "    #k_values = [2, 3, 4, 5, 6, 7, 8]\n",
    "    results = []\n",
    "\n",
    "    for k in vals:\n",
    "        start_time = time.time()\n",
    "        start_mem = get_memory_usage()\n",
    "        start_cpu = get_cpu_usage()\n",
    "\n",
    "        replacement_candidates, minhashes = func1(df_grouped, k, 0.98)\n",
    "        new_process_dictionary = func2(replacement_candidates)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        end_mem = get_memory_usage()\n",
    "        end_cpu = get_cpu_usage()\n",
    "\n",
    "        duration = end_time - start_time\n",
    "        mem_used = end_mem - start_mem\n",
    "\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'time_seconds': duration,\n",
    "            'memory_mb': mem_used,\n",
    "            'unique_processes': len(new_process_dictionary),\n",
    "            'cpu': end_cpu\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def plot_results(results):\n",
    "    k_values = [result['k'] for result in results]\n",
    "    time_seconds = [result['time_seconds'] for result in results]\n",
    "    cpu_percentages = [result['cpu'] for result in results]\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax1.set_xlabel('k values')\n",
    "    ax1.set_ylabel('Time (seconds)', color='tab:blue')\n",
    "    ax1.plot(k_values, time_seconds, marker='o', color='tab:blue', label='Time')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('CPU Usage (%)', color='tab:red')\n",
    "\n",
    "    ax2.plot(k_values, cpu_percentages, marker='^', color='tab:red', linestyle='--', label='CPU Usage')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    plt.title('Performance Metrics for Different k Values')\n",
    "    fig.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_performances(results):\n",
    "    k_values = [result['k'] for result in results]\n",
    "    time_seconds = [result['time_seconds'] for result in results]\n",
    "    cpu_percentages = [result['cpu'] for result in results]\n",
    "\n",
    "    # Calculate the performance metric (Product of time_seconds and cpu)\n",
    "    performance_metric = [time_seconds[i] * cpu_percentages[i] for i in range(len(results))]\n",
    "\n",
    "    # Create a figure and axis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the performance metric\n",
    "    plt.plot(k_values, performance_metric, marker='o', linestyle='-', color='purple', label='Time * CPU')\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('k values')\n",
    "    plt.ylabel('Performance Metric (Time * CPU)')\n",
    "    plt.title('Combined Metric of Time and CPU Usage vs. k Values')\n",
    "    plt.xticks(k_values)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def distinct_elements_in_order(lst):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "def shingle(text, k=7):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text)-k +1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return list(set(shingle_set))\n",
    "\n",
    "def jaccard_similarity(list1, list2):   \n",
    "    return len(set(list1).intersection(set(list2))) / len(set(list1).union(set(list2)))\n",
    "\n",
    "def minhash_lsh(df, k_shingle, threshold):\n",
    "\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
    "    minhashes = {}\n",
    "\n",
    "    for features in df.collect():\n",
    "        shingles = shingle(features[\"features\"], k_shingle)\n",
    "        m = MinHash(num_perm=128)\n",
    "        for shingle_item in shingles:\n",
    "            m.update(shingle_item.encode(\"utf8\"))\n",
    "        minhashes[int(features[\"user_id\"])] = m\n",
    "        lsh.insert(int(features[\"user_id\"]), m)\n",
    "\n",
    "    replacement_candidates = {}\n",
    "    for key in lsh.keys: \n",
    "        replacement_candidates[key] = lsh.query(minhashes[key]) \n",
    "\n",
    "    #Key: New representative, value: Similar items\n",
    "    return replacement_candidates,minhashes\n",
    "\n",
    "\n",
    "def remove_dups(graph):\n",
    "    keys_to_remove = []\n",
    "    for key, values in graph.items():\n",
    "        if key in values:\n",
    "            values.remove(key)\n",
    "        # Mark keys for removal if their value list is empty\n",
    "        if not values:\n",
    "            keys_to_remove.append(key)\n",
    "        else:\n",
    "            graph[key] = values  # Update the graph with the cleaned value list\n",
    "    \n",
    "    # Remove keys that had empty value lists\n",
    "    for key in keys_to_remove:\n",
    "        graph.pop(key)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def bucketing(replacement_candidates):\n",
    "    #replacement_candidates = remove_dups(replacement_candidates.copy())\n",
    "    visited_processes = set()\n",
    "    new_process_dictionary = {}\n",
    "\n",
    "    def bfs(start):\n",
    "        queue = [start]\n",
    "        bucket = []\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            if current not in visited_processes:\n",
    "                visited_processes.add(current)\n",
    "                bucket.append(current)\n",
    "                if current in replacement_candidates:\n",
    "                    for neighbor in replacement_candidates[current]:\n",
    "                        if neighbor not in visited_processes:\n",
    "                            queue.append(neighbor)\n",
    "        return bucket\n",
    "\n",
    "    for key in replacement_candidates.keys():\n",
    "        if key not in visited_processes:\n",
    "            bucket = bfs(key)\n",
    "            if bucket:\n",
    "                new_process_dictionary[key] = sorted(bucket)\n",
    "\n",
    "    return new_process_dictionary\n",
    "\n",
    "def get_case(caseID,data):\n",
    "    data1 = data.filter(data.user_id.isin([caseID]))\n",
    "    data1.show()\n",
    "\n",
    "def compare_cases(case1,case2,data):\n",
    "    data1 = data.filter(data.user_id.isin([case1]))\n",
    "    data2 = data.filter(data.user_id.isin([case2]))\n",
    "    desired_column_list1 = data1.select(\"to\").rdd.flatMap(lambda x: x).collect()\n",
    "    desired_column_list2 = data2.select(\"to\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    common_elements = np.intersect1d(desired_column_list1, desired_column_list2)\n",
    "    union_elements = np.union1d(desired_column_list1, desired_column_list2)\n",
    "    print(len(common_elements)/len(union_elements))\n",
    "\n",
    "def get_traces(user_id,df):\n",
    "    result = df.filter(col(\"user_id\") == user_id).select(\"features\").collect()\n",
    "    if result:\n",
    "        return result[0][\"features\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_shingles(user_id,df):\n",
    "    result = df.filter(col(\"user_id\") == user_id).select(\"shingles\").collect()\n",
    "    if result:\n",
    "        return result[0][\"shingles\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def write_output1(df, file_name):\n",
    "    # Ensure the Output directory exists\n",
    "    output_dir = 'Output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Format timestamp column to desired format\n",
    "    df = df.withColumn(\"timestamp\", date_format(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss.SSS\"))\n",
    "    \n",
    "    # Write DataFrame to temporary location\n",
    "    temp_output_dir = 'temp/temp_output'\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_output_dir)\n",
    "    \n",
    "    # Find the part file and move it to the desired location\n",
    "    part_file = [f for f in os.listdir(temp_output_dir) if f.startswith(\"part-\")][0]\n",
    "    shutil.move(os.path.join(temp_output_dir, part_file), os.path.join(output_dir, file_name))\n",
    "    \n",
    "    # Clean up temporary directory\n",
    "    shutil.rmtree(temp_output_dir)\n",
    "\n",
    "def write_output2(df, groups):\n",
    "    # Ensure the Output directory exists\n",
    "    os.makedirs('Output', exist_ok=True)\n",
    "\n",
    "    with open('Output/part2Observations.txt', 'w') as file:\n",
    "        for group_name, user_ids in groups.items():\n",
    "            # Write group header\n",
    "            file.write(f\"Group: {set(user_ids)}\\n\")\n",
    "            \n",
    "            for user_id in user_ids:\n",
    "                # Filter the DataFrame for the current user_id\n",
    "                user_df = df.filter(col(\"user_id\") == user_id)\n",
    "                \n",
    "                # Write user_id header\n",
    "                file.write(f\"{user_id}:\\n\")\n",
    "                \n",
    "                # Write each row for the current user_id\n",
    "                for row in user_df.collect():\n",
    "                    file.write(f\"< {row['from']}, {row['to']}, {row['timestamp']}, {row['type']}, {row['user_id']} >\\n\")\n",
    "                \n",
    "                # Add a newline for separation between users\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "def output(dataset, k, threshold,p1=True):\n",
    "    spark = SparkSession.builder.appName(\"OutputUserIDs\").getOrCreate()\n",
    "    data = spark.read.csv(dataset, header=True, inferSchema=True)\n",
    "    if p1 == True:\n",
    "        df_filtered = data.filter(data.type.isin(['Req']))\n",
    "        df_grouped = df_filtered.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"features\"))\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        df_filtered = data.filter((col(\"from\") == \"S0\") & (~col(\"to\").contains(\"null\")) & (~col(\"to\").contains(\"_\")))\n",
    "        df_grouped = df_filtered.groupBy(\"user_id\").agg(concat_ws(\"\",collect_list(\"to\")).alias(\"features\"))\n",
    "    \n",
    "    max_user_id = df_grouped.select(spark_max(\"user_id\")).collect()[0][0]\n",
    "    df_grouped.show()\n",
    "    \n",
    "        \n",
    "    replacement_candidates = minhash_lsh(df_grouped, k, threshold)\n",
    "    new_process_dictionary = bucketing(replacement_candidates[0])    \n",
    "\n",
    "    user_ids_to_change = []#[key for key in new_process_dictionary.keys()]#[key for key, values in new_process_dictionary.items() if len(values) > 1 or ]\n",
    "    for key, values in new_process_dictionary.items():\n",
    "        if len(values) >1:\n",
    "            user_ids_to_change.append(key)\n",
    "        elif len(values) == 1 and values[0]!= key:\n",
    "            user_ids_to_change.append(key)\n",
    "\n",
    "    user_ids_to_delete = []\n",
    "    for id in user_ids_to_change:\n",
    "        for case in new_process_dictionary[id]:\n",
    "            if case not in user_ids_to_change:\n",
    "                user_ids_to_delete.append(case)\n",
    "    \n",
    "    distinct_user_ids_df = df_grouped.select(\"user_id\").distinct()\n",
    "    distinct_user_ids = set([row.user_id for row in distinct_user_ids_df.collect()])\n",
    "\n",
    "    user_ids_to_maintain = distinct_user_ids-set(user_ids_to_delete)-set(user_ids_to_change)\n",
    "    maintain_df = data.filter(col(\"user_id\").isin(list(user_ids_to_maintain)))  \n",
    "    \n",
    "    change_df = data.filter(col(\"user_id\").isin(user_ids_to_change))\n",
    "\n",
    "    distinct_user_ids_to_change = change_df.select(\"user_id\").distinct()\n",
    "    window_spec = Window.orderBy(\"user_id\")\n",
    "    user_id_mapping = distinct_user_ids_to_change.withColumn(\"new_user_id\", row_number().over(window_spec) + max_user_id )\n",
    "    df_with_new_ids = data.join(user_id_mapping, on=\"user_id\", how=\"right\")\n",
    "    df_with_new_ids = df_with_new_ids.withColumn(\"user_id\",\n",
    "                                                 when(col(\"new_user_id\").isNotNull(), col(\"new_user_id\"))\n",
    "                                                 .otherwise(col(\"user_id\"))) \\\n",
    "                                     .drop(\"new_user_id\")\n",
    "    columns = df_with_new_ids.columns\n",
    "    columns.remove('user_id')\n",
    "    new_column_order = columns + ['user_id']\n",
    "    df_with_new_ids = df_with_new_ids.select(new_column_order)\n",
    "\n",
    "    final_df = df_with_new_ids.union(maintain_df)\n",
    "    if p1== True:\n",
    "        write_output1(final_df, 'part1Output.txt')\n",
    "    write_output2(data, new_process_dictionary)\n",
    "\n",
    "\n",
    "def get_averege_jaccard_sim(final_buckets, minhashes,get = True):\n",
    "    sims = {}\n",
    "    for key, value in final_buckets.items():\n",
    "        for user_id_1 in final_buckets[key]:\n",
    "            for user_id_2 in final_buckets[key]:\n",
    "                if user_id_1 != user_id_2:\n",
    "                    sig_1 = minhashes[int(user_id_1)]\n",
    "                    sig_2 = minhashes[int(user_id_2)]\n",
    "                    sim = MinHash.jaccard(sig_1, sig_2)\n",
    "                    if key not in sims:\n",
    "                        sims[key] = [sim]\n",
    "                    else:\n",
    "                        sims[key].append(sim)\n",
    "    print(sims)\n",
    "    total_sum = 0\n",
    "    total_count = 0\n",
    "    sims = dict(sorted(sims.items()))\n",
    "    if get == True:\n",
    "        for key, value in sims.items():\n",
    "            avg_sim = average(value)\n",
    "            print(key, avg_sim)\n",
    "            total_sum += sum(value)\n",
    "            total_count += len(value)\n",
    "        \n",
    "        overall_average = total_sum / total_count if total_count != 0 else 0\n",
    "        print(\"Overall Average Jaccard Similarity:\", overall_average)\n",
    "\n",
    "    return sims\n",
    "\n",
    "tasks = {'log_in': ['one',['credentials check','sing up','recover pw and log in'],(0,10)],\n",
    "            'search_book': ['rand',['history','fantasy','crime','poetry','biography'],(5,15)],\n",
    "            'shipment' : ['all',['adress','door number','zip code'],(15,50)],\n",
    "            'payment' : ['one',['visa','master card','revolut','paypal','apple pay'],(10,100)],\n",
    "            'new_site' : ['con',['site1','site2'],(10,100)],\n",
    "            'site1' : ['opt',['site1_1','site1_2','site1_3'],(10,100)],\n",
    "            'site2' : ['opt',['site2_1','site2_2'],(10,100)]\n",
    "}   \n",
    "start_time = datetime(2024, 6, 3, 9, 0, 0)  \n",
    "end_time = datetime(2024, 6, 3, 10, 45, 0)\n",
    "#generate_dataset(tasks, 1000,start_time,end_time,file_name=\"dataset_test\")  \n",
    "#output(\"data/SDG_dataset_test.csv\", 7, 0.5,p1=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 12:24:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/tomascarrilho/Desktop/Uni/DIS/Assignment/Data_Intensive_Systems/Old/data/SDG_dataset2.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilter Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m----> 2\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/SDG_dataset2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df_filtered2 \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39mfilter((col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnull\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      4\u001b[0m df_grouped2 \u001b[38;5;241m=\u001b[39m df_filtered2\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(concat_ws(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,collect_list(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/tomascarrilho/Desktop/Uni/DIS/Assignment/Data_Intensive_Systems/Old/data/SDG_dataset2.csv."
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Filter Dataset\").getOrCreate()\n",
    "df2 = spark.read.csv(\"data/SDG_dataset2.csv\", header=True, inferSchema=True)\n",
    "df_filtered2 = df2.filter((col(\"from\") == \"S0\") & (~col(\"to\").contains(\"null\")) & (~col(\"to\").contains(\"_\")))\n",
    "df_grouped2 = df_filtered2.groupBy(\"user_id\").agg(concat_ws(\"\",collect_list(\"to\")).alias(\"features\"))\n",
    "\n",
    "\n",
    "\n",
    "def shingle2(text, k=3):\n",
    "    \n",
    "    shingle_set = []\n",
    "    for i in range(len(text)-k +1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    #ans = [i for i in shingle_set if i not in ans]\n",
    "    return distinct_elements_in_order(shingle_set)\n",
    "\n",
    "shingles_udf2 = udf(shingle2, ArrayType(StringType()))\n",
    "df_shingles2 = df_filtered2.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"trace\")) \\\n",
    "    .withColumn(\"shingles\", shingles_udf2(col(\"trace\"))) \\\n",
    "    .select(\"user_id\", \"shingles\")\n",
    "\n",
    "average_length = df_grouped2.select(avg(length(col('features')))).collect()[0][0]\n",
    "average_shingles = df_shingles2.select(avg(size(col('shingles')))).collect()[0][0]\n",
    "\n",
    "#filtered_df.show()\n",
    "#df_grouped2.show()\n",
    "#df_shingles2.show()\n",
    "# print(df_grouped2.collect()[0][1])\n",
    "# print(df_shingles2.collect()[0][1])\n",
    "print('average trace length:',average_length)\n",
    "print('average trace shingles:',average_shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of cases: 43566\n",
      "Number of unique processes after merging them with 0.97 threshold using 7-shingles: 320\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial number of cases: {df_grouped2.count()}\")\n",
    "ans = minhash_lsh(df_grouped2,3,0.97)\n",
    "replacement_candidates7, minhash_dic7 = ans[0],ans[1]\n",
    "new_process_dictionary7= bucketing(replacement_candidates7)\n",
    "print(f\"Number of unique processes after merging them with 0.97 threshold using 7-shingles: {len(new_process_dictionary7)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_process_dictionary7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3S4S2S1\n",
      "S3S4S2S1\n"
     ]
    }
   ],
   "source": [
    "print(get_traces(1,df_grouped2))\n",
    "print(get_traces(999,df_grouped2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 10:57:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "generate_dataset(tasks, 100,start_time,end_time,file_name=\"dataset2\") \n",
    "output(\"data/SDG_dataset2.csv\", 3, 0.97,p1=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_dataset(tasks, 1000,start_time,end_time,file_name=\"dataset2\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3,5,7,9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(text, k=7):\n",
    "    shingle_set = []\n",
    "    for i in range(len(text)-k +1):\n",
    "        shingle_set.append(text[i:i+k])\n",
    "    return list(set(shingle_set))\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\"data/SDG_dataset_test.csv\", header=True, inferSchema=True)\n",
    "df_filtered_m = data.filter(data.type.isin(['Req']))\n",
    "df_grouped = df_filtered_m.groupBy(\"user_id\").agg(concat_ws(\"\",collect_list(\"to\")).alias(\"features\"))\n",
    "\n",
    "shingles_udf = udf(shingle, ArrayType(StringType()))\n",
    "df_shingles = df_filtered_m.groupBy(\"user_id\").agg(concat_ws(\"\", collect_list(\"to\")).alias(\"trace\")) \\\n",
    "    .withColumn(\"shingles\", shingles_udf(col(\"trace\"))) \\\n",
    "    .select(\"user_id\", \"shingles\")\n",
    "\n",
    "#data.show()\n",
    "#df_filtered_m.show()\n",
    "#df_grouped.show()\n",
    "#df_shingles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg number of shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32.45076908906696"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_num_shingles = df_shingles.withColumn(\"list_length\", size(col(\"shingles\"))) \\\n",
    "                     .agg(avg(\"list_length\").alias(\"average_list_length\")).collect()[0][0]\n",
    "average_num_shingles\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do (avg-1)/avg and use thar as threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of cases: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "PySparkValueError",
     "evalue": "features",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/types.py:2364\u001b[0m, in \u001b[0;36mRow.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2362\u001b[0m     \u001b[38;5;66;03m# it will be slow when it has many fields,\u001b[39;00m\n\u001b[1;32m   2363\u001b[0m     \u001b[38;5;66;03m# but this will not be used in normal cases\u001b[39;00m\n\u001b[0;32m-> 2364\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__fields__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(Row, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(idx)\n",
      "\u001b[0;31mValueError\u001b[0m: 'features' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial number of cases: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_grouped\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mminhash_lsh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_shingles\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.97\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m replacement_candidates7, minhash_dic7 \u001b[38;5;241m=\u001b[39m ans[\u001b[38;5;241m0\u001b[39m],ans[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m new_process_dictionary7\u001b[38;5;241m=\u001b[39m bucketing(replacement_candidates7)\n",
      "Cell \u001b[0;32mIn[6], line 111\u001b[0m, in \u001b[0;36mminhash_lsh\u001b[0;34m(df, k_shingle, threshold)\u001b[0m\n\u001b[1;32m    108\u001b[0m minhashes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcollect():\n\u001b[0;32m--> 111\u001b[0m     shingles \u001b[38;5;241m=\u001b[39m shingle(\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, k_shingle)\n\u001b[1;32m    112\u001b[0m     m \u001b[38;5;241m=\u001b[39m MinHash(num_perm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shingle_item \u001b[38;5;129;01min\u001b[39;00m shingles:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/types.py:2369\u001b[0m, in \u001b[0;36mRow.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(item)\n\u001b[1;32m   2368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m-> 2369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(item)\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: features"
     ]
    }
   ],
   "source": [
    "print(f\"Initial number of cases: {df_grouped.count()}\")\n",
    "ans = minhash_lsh(df_grouped,7,0.97)\n",
    "replacement_candidates7, minhash_dic7 = ans[0],ans[1]\n",
    "new_process_dictionary7= bucketing(replacement_candidates7)\n",
    "print(f\"Number of unique processes after merging them with 0.97 threshold using 7-shingles: {len(new_process_dictionary7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the worst case scneario, with the smallest approximate jaccard sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = get_averege_jaccard_sim(replacement_candidates7, minhash_dic7,get=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dissimilar gives you the key for the buckets with the smallest jaccard sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = min(set(value for key,values in sims.items() for value in values if value != 1.0))\n",
    "final_values = []\n",
    "for key,values in sims.items():\n",
    "    for value in values:\n",
    "        if value == ans:\n",
    "            final_values.append(key)\n",
    "\n",
    "dissimilar = set(final_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the minimum Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8828125\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THese are the cases of which the buckets have the sim above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{43040, 1250, 42562, 23829, 17751}\n"
     ]
    }
   ],
   "source": [
    "print(dissimilar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANd here we get all the user_ids that have the smaller similarities and their key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_sims = []\n",
    "for key in dissimilar:\n",
    "    for value in replacement_candidates7[key]:\n",
    "        new_sims.append((key,value,jaccard_similarity(get_shingles(value,df_shingles),get_shingles(key,df_shingles))))\n",
    "investigate = [case for case in new_sims if case[-1]!=1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### 43040 vs 1250 ################################\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n",
      "#######################################################################\n",
      "######################### 43040 vs 23829 ################################\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n",
      "#######################################################################\n",
      "######################### 1250 vs 43040 ################################\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n",
      "#######################################################################\n",
      "######################### 1250 vs 42562 ################################\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n",
      "#######################################################################\n",
      "######################### 1250 vs 17751 ################################\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n",
      "#######################################################################\n",
      "######################### 42562 vs 1250 ################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n",
      "#######################################################################\n",
      "######################### 42562 vs 23829 ################################\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n",
      "#######################################################################\n",
      "######################### 23829 vs 43040 ################################\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n",
      "#######################################################################\n",
      "######################### 23829 vs 42562 ################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n",
      "#######################################################################\n",
      "######################### 23829 vs 17751 ################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n",
      "#######################################################################\n",
      "######################### 17751 vs 1250 ################################\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n",
      "#######################################################################\n",
      "######################### 17751 vs 23829 ################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_4\n",
      "S0S4S4_1S3S3_1S3_2S3_3S2S2_2\n",
      "#######################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for case in investigate:\n",
    "    print(f'######################### {case[0]} vs {case[1]} ################################')\n",
    "    print(get_traces(case[0],df_grouped))\n",
    "    print(get_traces(case[1],df_grouped))\n",
    "    print('#######################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of what I saw with Hugo. Repeat all this for each k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramter-tuning for k-shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to see which k is the best for the k-shingles. We gonna do such a thing by investigating how computational expensive it is to compute such minhash for different values of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_performance(minhash_lsh,bucketing, [3,4,5,6,7,8,9])\n",
    "plot_results(results)\n",
    "plot_performances(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running this a considerable amount of times we saw that the value that get a better balance between time and CPU usage is when k=7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter-tuning for the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the average number of 7-shingles is 32, and we want to group processes with only small variations, we want that 31/32 shingles to be the same, so that we still allow for some small variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial number of cases: {df_grouped.count()}\")\n",
    "ans = minhash_lsh(df_grouped,7,0.97)\n",
    "replacement_candidates7, minhash_dic = ans[0],ans[1]\n",
    "new_process_dictionary7= bucketing(replacement_candidates7)\n",
    "print(f\"Number of unique processes after merging them with 0.97 threshold using 7-shingles: {len(new_process_dictionary7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the processes with a threshold of 97%, using 7-shingles, we obtain 29729 candidate unique cases. In order to investigate if further analysis into the similarities needs to be done, to make sure that the false positives do not result in cases where the cases are not small variations of each other, we are going to compute the average similarities of all buckets and investigate the mininum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = get_averege_jaccard_sim(replacement_candidates7, minhash_dic,get=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to mention that we're still just computing the approximate jaccard similarities provided by MinHashLSH, so, in order to investigate the cases that have the smallest approximate jaccard similarities, we're going to compute the actual similarities between those cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = min(set(value for key,values in sims.items() for value in values if value != 1.0))\n",
    "final_values = []\n",
    "for key,values in sims.items():\n",
    "    for value in values:\n",
    "        if value == ans:\n",
    "            final_values.append(key)\n",
    "\n",
    "dissimilar = set(final_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in investigate:\n",
    "    print(f'######################### {case[0]} vs {case[1]} ################################')\n",
    "    print(get_traces(case[0],df_grouped))\n",
    "    print(get_traces(case[1],df_grouped))\n",
    "    print('#######################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_part1(\"data/SDG_dataset2.csv\",3,0.95)\n",
    "# output_part1(\"data/SDG_dataset2.csv\",5,0.95)\n",
    "output_part1(\"data/SDG_dataset2.csv\",7,0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning for Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "minhashes = []\n",
    "user_ids = []\n",
    "final_buckets = {}\n",
    "for features in df_grouped.collect():\n",
    "    shingles = shingle(features[\"features\"], 5)\n",
    "    m = MinHash(num_perm=128)\n",
    "    for shingle_item in shingles:\n",
    "        m.update(shingle_item.encode(\"utf8\"))\n",
    "    minhashes.append(m.hashvalues)\n",
    "    user_ids.append(int(features[\"user_id\"]))\n",
    "\n",
    "param_grid = {\n",
    "    'n_clusters': [100, 250, 500],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "}\n",
    "\n",
    "kmeans = KMeans()\n",
    "\n",
    "grid_search = GridSearchCV(kmeans, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(minhashes)\n",
    "\n",
    "best_param = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"The best parameters are: \" , best_param )\n",
    "print(\"The best model is: \", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Find and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_candidates = minhash_lsh(df_grouped,5,0.7)\n",
    "new_process_dictionary = bucketing(replacement_candidates)\n",
    "print(len(replacement_candidates))\n",
    "print(len(new_process_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Find/cluster similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "users = []\n",
    "for key in new_process_dictionary:\n",
    "    users.append(key)\n",
    "\n",
    "filtered_df = df_grouped[df_grouped['user_id'].isin(users)]\n",
    "\n",
    "final_buckets, minhashes = kmeans_clustering(filtered_df,500,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_averege_jaccard_sim(final_buckets, minhashes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Initial number of cases: {df_grouped.count()}\")\n",
    "# replacement_candidates3 = minhash_lsh(df_grouped,3,0.98)\n",
    "# new_process_dictionary3 = bucketing(replacement_candidates3)\n",
    "#print(f\"After merging cases with threshold 3-shingles: {len(new_process_dictionary3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split = df_grouped.withColumn(\"feature_length\", size(split(col(\"features\"), \"S\")))\n",
    "# average_length = df_split.agg(avg(\"feature_length\")).collect()[0][0]\n",
    "\n",
    "# windowSpec = Window.partitionBy(F.lit(1)).orderBy(\"feature_length\")\n",
    "# df_split = df_split.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "# total_count = df_split.count()\n",
    "\n",
    "# if total_count % 2 == 0:\n",
    "#     median_index1 = total_count // 2\n",
    "#     median_index2 = median_index1 + 1\n",
    "#     median_value = df_split.filter(col(\"row_number\").isin([median_index1, median_index2])) \\\n",
    "#                                   .agg(avg(\"feature_length\")).collect()[0][0]\n",
    "# else:\n",
    "#     median_index = (total_count // 2) + 1\n",
    "#     median_value = df_split.filter(col(\"row_number\") == median_index) \\\n",
    "#                                   .select(\"feature_length\").collect()[0][0]\n",
    "    \n",
    "# print(f\"Average number of requests: {average_length}\")\n",
    "# print(f\"Median number of requests: {median_value}\")\n",
    "\n",
    "\n",
    "#Given that both the average and the median are close to 13, which shows that the dataset is symetricly distributed when it comes to how many \n",
    "#requests are performed, we gonna assume that two cases have a small variation iif the number of different requests is around 1. To get an \n",
    "# #approximation of the threshold, we're going to use the resutls shown before, so we assume that 12/13 are the same. Given this, we decided \n",
    "# to use a threshold of 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_node_from_neighbors(graph, node_to_remove):\n",
    "    if node_to_remove in graph:\n",
    "        # Remove node_to_remove from all adjacency lists in the graph\n",
    "        for node, neighbors in graph.items():\n",
    "            if node != node_to_remove:  # Skip the node_to_remove itself\n",
    "                if node_to_remove in neighbors:\n",
    "                    neighbors.remove(node_to_remove)\n",
    "\n",
    "def dfs(graph, start, visited):\n",
    "    stack = [start]\n",
    "    result = []\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            result.append(node)\n",
    "            stack.extend(graph[node])\n",
    "    return result\n",
    "\n",
    "def transform_graph(graph):\n",
    "    reachable_dict = {}\n",
    "    for node in graph:\n",
    "        visited = set()\n",
    "        reachable_nodes = dfs(graph, node, visited)\n",
    "        reachable_dict[node] = [n for n in reachable_nodes if n != node]  # Exclude the start node itself\n",
    "    \n",
    "    # Filter out keys that also appear as values\n",
    "    keys_to_remove = set()\n",
    "    for node, neighbors in graph.items():\n",
    "        keys_to_remove.update(neighbors)\n",
    "    \n",
    "    final_dict = {k: v for k, v in reachable_dict.items() if k not in keys_to_remove}\n",
    "    return final_dict\n",
    "\n",
    "def bucketing(old_replacement_candidates):\n",
    "    alt_old_replacement_candidates = old_replacement_candidates.copy()\n",
    "    replacement_candidates = alt_old_replacement_candidates\n",
    "    print('no dups')\n",
    "    print(replacement_candidates)\n",
    "    visited = set()\n",
    "    for node in replacement_candidates.keys():\n",
    "        if node not in visited:\n",
    "            remove_node_from_neighbors(replacement_candidates, node)\n",
    "        for neighbour in replacement_candidates[node]:\n",
    "            visited.add(neighbour)\n",
    "    # print('no neighbours')\n",
    "    # print(replacement_candidates)\n",
    "    new_process_dictionary = transform_graph(replacement_candidates)\n",
    "    print('final')\n",
    "    print(new_process_dictionary)\n",
    "    \n",
    "        \n",
    "    return new_process_dictionary\n",
    "\n",
    "#generate_dataset(tasks, 100000,start_time,end_time,file_name=\"dataset1\",random=False,connect=False)  \n",
    "#print('minhash')\n",
    "ans = output_part1(\"data/SDG_dataset1.csv\", 5, 0.97)\n",
    "#print(ans[39])\n",
    "print(ans)\n",
    "#ans2 = bucketing(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
